% Author: Clara Eleonore Pavillet
% Version: 1.0
% This work is licensed under a Creative Commons Attribution 4.0 International License.

\documentclass[12pt, a4paper]{report}
\input{Packages.tex}
\hypersetup{pdftitle = Thesis, pdfauthor = {First Last}, pdfstartview=FitH, pdfkeywords = essay, pdfpagemode=FullScreen, colorlinks, anchorcolor = red, citecolor = blue, urlcolor=blue, filecolor=green, linkcolor=red, plainpages=false}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\chead{}
\lhead{University of Oxford}
\lfoot{\date{\today}}
\cfoot{}
\rfoot{\thepage}
% Top and Bottom Line Rules
\renewcommand{\headrulewidth}{0.4pt} %0.4pt
\renewcommand{\footrulewidth}{0.4pt}
\fancyheadoffset{9pt}
\fancyfootoffset{9pt}
% Line spacing
\renewcommand{\baselinestretch}{1.2} %1.5
% table size
\renewcommand{\arraystretch}{1.4}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\date{24.04.22}

\title{Online Learning, Non-stationary Environment, and Dynamic Regret}
\author{\\ %\Large{Wonsuk Yang}
\\
\\
\\ Candidate Number: 1066480
\\
\\ University of Oxford
\\
A dissertation submitted for the degree of \\ \textit{Master of Science}
\\ \\
Trinity 2022
\\ 
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
% Adjust logo positions here
\AddToShipoutPicture*{\BackgroundPicturea{Logos/logo2.png}{0.7in}{5.8in}}
\thispagestyle{headings}
	\maketitle
\FloatBarrier
\pagenumbering{roman}

\thispagestyle{empty}
%
%% !!!! ABSTRACT HERE !!!!
%
\begin{abstract}
The main topic of this work is dynamic regret and non-stationary environment. Unlike conventional online learning framework, where the environment is assumed to follow a fixed policy, in practice environment changes its strategy as the time progresses. In such a case, static regret, a difference between cumulative loss of the player and that incurred by the best decision in hindsight, may lead to sub-optimal algorithm.

Dynamic regret is a generalization of ordinary regret where the player's strategy is compared against a comparator sequence. The change in environment can be modelled through the comparator, making dynamic regret a reasonable objective when taking non-stationary environment into account. 

The main goal of this work is to introduce dynamic regret and discuss algorithms which allow sub-linear regret bound that depends on a measure of shifts within the comparator. While standard algorithms like online gradient descent cannot attain minimax optimal regret bound without knowing path-length of the comparator, this issue can be resolved by aggregating results from algorithms which runs in parallel but with different learning rates. We then apply this technique to new setting: prediction with expert advice problem.

% \keywords{Keyword1 - Keyword2 - Keyword3}
% % \vspace{-10mm} %To remove added white space after
\end{abstract}
\tableofcontents
% \thispagestyle{plain}
% \listoffigures
% \listoftables

% \chapter*{List of Abbreviations}
% \begin{abbreviations}
%     \item[MAS] Multi-Agent System
% \end{abbreviations}

\chapter{Introduction}
\pagenumbering{arabic}
\section{Motivation}
Decision making problems are prevalent across multiple areas of research and application, and in many cases, we are suppose to choose the best action "on the fly", that is, we make our decision sequentially based on the streaming data. Online learning is a field of research about design and analysis of strategies that tackle these sequential decision making problems. Online learning framework generalizes multiple real-life problems: recommendation systems, online portfolio selection, and online shortest path to name a few.


A concept that stands on the opposite side of online learning is offline (batch) learning. Here, the data is collected and stored beforehand, and hence is considered to be static. Thus, online learning problem adds an extra challenge to offline learning problem in that our strategy must be able to forecast an unseen adversary.

In online learning framework we typically assume that the environment is \textit{oblivious}, that is, the losses are sampled from a \textbf{fixed} distribution before the player makes any decision. However, the assumption is often unrealistic in practice, since the distribution from which the losses are sampled are likely to change as the time progresses, $i.e.,$ our environment is non-stationary. 

In conventional online learning setting, the effectiveness of player's strategy is measured by (static) regret, which compares the strategy against the best decision in hindsight. Algorithm with small static regret may converge to a fixed decision, which is sub-optimal when up against dynamic environment. Dynamic regret, which measures the cumulative loss incurred by the player's strategy to that by an arbitrary sequence of actions, is a generalization of static regret. This is a reasonable alternative of static regret when the environment is changing, since we can allow the comparator to model the non-stationary behavior. 

While standard algorithms like online gradient descent can guarantee sub-linear dynamic regret bound, they require a priori knowledge of the comparator sequence. To reiterate the challenge, we want a regret bound that holds for any comparator sequence but to obtain tighter bound, the algorithm must be tailored to specific sequence. In this work, we investigate work of Zhang et. al. \cite{Zhang2018AdaptiveOL}, which presents an algorithm which is both oblivious to the comparator sequence and yet guarantees regret bound which is minimax optimal. While their work is tailored to solving general online convex optimization problem using online gradient descent, we seek to generalize their work so that we can apply their technique to a broader class of online convex optimization problem settings and algorithms.

The contribution of this work is the following.
\begin{itemize}
    \item Present a motivation and background for dynamic regret and algorithm \ref{ader:controller}, which attains minimax optimal upper bound. 
    \item Generalize results of Zenkenvich \cite{Zinkevich2003OnlineCP} and Zhang et. al. \cite{Zhang2018AdaptiveOL} to online mirror descent setting.
    \item Apply technique in Zhang et. al. \cite{Zhang2018AdaptiveOL} to tracking the best expert problem.
\end{itemize}



\section{Outline}
Chapter \hyperref[Chap2]{2} is about online convex optimization. We start by introducing elementary convex analysis and move onto algorithms that under certain assumptions can guarantee sub-linear regret bound. The highlight of chapter \hyperref[Chap2]{2} is \textbf{online mirror descent}, which generalizes well-known online gradient descent. In chapter \hyperref[Chap3]{3}, we introduce our main topic - dynamic regret. We prove lower bound on dynamic regret for vanilla online convex optimization problem and show that online mirror descent can attain minimax optimal regret bound with the knowledge of path length of comparator.  
In chapter \hyperref[Chap4]{4} we give a summary of our work and list the paths for future investigation.

\chapter{Online Convex Optimization}
\label{Chap2}
\section{Convex Optimization}
In this section, we review basic concepts of convex analysis. We begin by defining convex set and convex function.
\begin{defn}
A set $ \mathcal{C} \subseteq \mathbb{R}^{d} $ is convex if for all $\mathbf{x}, \mathbf{y} \in \mathcal{C}$ and $\alpha \in [0, 1]$, $\alpha \mathbf{x} + (1 - \alpha) \mathbf{y} \in \mathcal{K}$.
\end{defn}
\begin{defn}
Let $\mathcal{C}$ be a convex subset of $\mathbb{R}^d$. A function $f: \mathcal{C} \rightarrow \mathbb{R}$ is convex if for all $\mathbf{x}, \mathbf{y} \in \mathcal{C}$ and $\lambda \in [0, 1]$,
\begin{center}
    $f(\lambda \mathbf{x} + (1-\lambda) \mathbf{y}) \leq \lambda f(\mathbf{x}) + (1 - \lambda)f(\mathbf{y})$
\end{center}
The function is strictly convex if the above inequality is strict for $\mathbf{x} \neq \mathbf{y}$.
\end{defn}

\begin{defn}
Let $\mathcal{C}$ be a subset of $\mathbb{R}^d$ and $f: \mathcal{C} \rightarrow \mathbb{R}$ be a function. A sub-gradient of $f$ at $\mathbf{x}$ is a vector $g \in \mathbb{R}^d$ such that
\begin{center}
    $f(\mathbf{y}) \geq f(\mathbf{x}) + g^\top(\mathbf{y} - \mathbf{x})$
\end{center}
for all $\mathbf{y} \in \mathcal{C}$. The set of sub-gradients of $f$ at $\mathbf{x}$ is denoted $ \partial f(\mathbf{x})$.
\end{defn}

Next proposition tells that sub-gradient is closely related convexity, and with additional assumption of smoothness, sub-gradient is nothing more than a gradient.
\begin{prop} \label{prop:grad-is-subgrad}
Let $f: \mathcal{C} \rightarrow \mathbb{R}$ be convex. Then, $\partial f(\mathbf{x}) \neq \varnothing$ for all $\mathbf{x} \in int(\mathcal{C})$. Moreover, if $f$ is differentiable at $\mathbf{x}$, then $\partial f(\mathbf{x}) = \{\nabla f(\mathbf{x})\}$.
\end{prop}
Thus, sub-gradient is a generalization of the gradient of a function in the context of convex analysis. From now on, we will always assume that all functions are differentiable as well as convex, unless stated otherwise.  

Geometrically speaking, proposition \ref{prop:grad-is-subgrad} states that the graph of a convex function is always above the tangent point of any point $\mathbf{x} \in \mathcal{C}$. We have stronger notion of convexity, namely strong convexity, which roughly says that at each point there is a quadratic by which the function is bounded below. 
\begin{defn}
Let $\mathcal{C}$ be a convex subset of $\mathbb{R}^d$. A smooth function $f : \mathcal{C} \rightarrow \mathbb{R}$ is $\lambda$-strongly convex with respect to a norm $\lVert \cdot \rVert$ if there is a parameter $\lambda > 0$ such that for all $\mathbf{x}, \mathbf{y} \in \mathcal{C}$, 
\begin{center}
    $\displaystyle f(\mathbf{y}) \geq f(\mathbf{x}) + \langle \nabla f(\mathbf{x}), \mathbf{y} - \mathbf{x} \rangle + \frac{\lambda}{2}\lVert \mathbf{x} - \mathbf{y} \rVert^2$
\end{center}
\end{defn}
Note it is clear from definition that strongly convexity implies strictly convexity.

\section{Online Convex Optimization}
Online learning is a well-established machine learning framework that tackles problems where data comes in a sequence. 
The key difference between online learning framework and its offline counterpart is that the player makes a decision and receive feedback from the environment (adversary) \textbf{at each round}.

The heart of online learning is devising an algorithm that makes a sequence of accurate predictions. To this end, we restrict to online convex optimization (OCO) problems to use tools from convex optimization. For detailed survey of this topic, refer to \cite{Orabona2019OnlineLearning, ShalevShwartz2007OnlineLT}.

\begin{defn}[Online Convex Optimization \cite{Zinkevich2003OnlineCP}]
Let $\mathcal{X} \in \mathbb{R}^d$ be a non-empty closed convex set of feasible actions and $\mathcal{F} = \{f : \mathcal{X} \rightarrow \mathbb{R} \}$ a subset of convex loss functions. For each round $t=1,2,...,T$, 
\begin{itemize}
    \item Player makes a decision $\mathbf{x}_t \in \mathcal{X}$
    \item Adversary chooses $f_t \in \mathcal{F}$
    \item Player suffers a loss $f_t(\mathbf{x}_t)$, and observes some information about $f_t$
\end{itemize}
The goal of the player is to minimize (static) \textbf{regret}
\begin{center}
    $\displaystyle Reg_T^{S} = \sum_{t=1}^{T} f_t(\mathbf{x}_t) - \min_{\mathbf{x} \in \mathcal{X}} \sum_{t=1}^{T} f_t(\mathbf{x})$
\end{center}
\end{defn}
We have different problem setting depending on how the loss functions are selected.
\begin{itemize}
    \item Stochastic: $f_1, \dots, f_T$ are drawn as $i.i.d.$ samples from a fixed distribution.
    \item Oblivious: $f_1, \dots, f_T$ can be arbitrary, but chosen before the player makes any decision (in particular, they do not depend on player's decisions)
    \item Adaptive: For all $t \geq 1$, $f_t$ depends on player's actions prior to $t$ 
\end{itemize}
Depending on what information we gain about the loss function, we have two different settings.
\begin{itemize}
    \item Full Information: Player observes $f_t$, or more commonly, $\nabla f_t(\mathbf{x}_t)$
    \item Bandit: Player only observes $f_t(\mathbf{x}_t)$
\end{itemize}
Unless stated otherwise, we will assume \textit{oblivious} adversary and \textit{full information} setting. 

Regret measures the difference between total loss incurred by the player and that incurred by a single best action in hindsight. The player is "successful" if in the long run, the average regret goes to 0, $i.e.\ Reg_{T}^S = o(T)$.
Thus, the main objective in online learning is to propose an algorithm which produces a sequence $(\mathbf{x}_t)_{t \geq 1}$ that guarantees sub-linear regret bound.

\subsection{Online Gradient Descent}
Gradient descent is a powerful algorithm in convex analysis for finding a minimum of a function $f$. The idea behind gradient descent can be summarized as the following:
\begin{equation*}
    \mathbf{x}_{t+1}^\prime = \mathbf{x}_t - \eta_t \nabla f(\mathbf{x}_t)
\end{equation*}
\begin{equation*}
    \mathbf{x}_{t+1} = \Pi_\mathcal{X} (\mathbf{x}_{t+1}^\prime).
\end{equation*}    
$\eta_t > 0$ is called step size at time step $t$ and determines how much we move against the gradient. $\displaystyle \Pi_\mathcal{X}(\mathbf{x}) = \argmin_{\mathbf{y} \in \mathcal{X}} \lVert \mathbf{y} - \mathbf{x} \rVert_{2}$ is the projection operator, which yields a point in the feasible set $\mathcal{X}$ closest to a given point $\mathbf{x} \in \mathbb{R}^d$. 

Online gradient descent (OGD), an online version of gradient descent first introduced in \cite{Zinkevich2003OnlineCP}, is an elementary yet effective algorithm for online convex optimization. 
\begin{algorithm}
\caption{Online Gradient Descent}\label{alg:ogd}
\begin{algorithmic}[1]
\Require{Stopping time $T$, initial value $\mathbf{x}_1 \in \mathcal{X}$, step sizes $(\eta_t)_{t \geq 0}$}
\For {$t = 1, ..., T$}
\State Player makes decision $\mathbf{x}_t$
\State Player suffers loss $f_t(\mathbf{x}_t)$, observes $f_t$
\State Update $\mathbf{x}_{t+1} = \Pi_{\mathcal{X}} (\mathbf{x}_t - \eta_t \nabla f_t(\mathbf{x}_t))$
\EndFor
\end{algorithmic}
\end{algorithm}

We are interested in whether online gradient descent guarantees sub-linear regret bound. If the feasible set $\mathcal{X}$ is bounded and the loss functions $(f_t)_{t=1}^T$ are Lipschitz continuous, the answer is affirmative.

\begin{prop} \label{prop:ogd-bound}
Consider online gradient descent with constant step size, $i.e. \eta_t = \eta$ for all $t = 1, \dots, T$. Take any starting point $\mathbf{x}_1 \in \mathcal{X}$ as initial value. Let $\mathbf{x}^{*} \in \argmin_{\mathbf{x} \in \mathcal{X}} \sum_{t=1}^{T} f_t(\mathbf{x})$. Then, the following regret bound holds 
\begin{center}
    $\displaystyle Reg_T^S \leq \frac{\lVert \mathbf{x}_1 - \mathbf{x}^{*} \rVert_2^2}{2\eta} + \frac{\eta}{2}\sum_{t=1}^{T} \lVert \nabla f_t(\mathbf{x}_t) \rVert_2^2$
\end{center}
In particular, if $\lVert \mathbf{x} - \mathbf{y} \rVert_2 \leq D$ and $\lVert \nabla f(\mathbf{x}) \rVert_2 \leq G$ for all $\mathbf{x}, \mathbf{y} \in \mathcal{X}$ and $f \in \mathcal{F}$, then OGD attains sub-linear regret bound by choosing $\eta \propto \frac{1}{\sqrt{T}}$. If so, we have
\begin{center}
    $Reg_T^S = O(\sqrt{T})$
\end{center}
\end{prop}

\subsection{Mirror Descent}

It is evident from the definition of OGD and proposition \ref{alg:ogd} that there is a close tie between OGD and $l_2$ norm.
However, not all problem can be effectively modelled using $l_2$ norm. For instance, suppose our feasible set $\mathcal{X} \subset \mathbb{R}^d$ is 1-ball with respect to $l_\infty$ norm. Then, $\sup_{\mathbf{x}, \mathbf{y} \in \mathcal{X}} \lVert \mathbf{x} - \mathbf{y} \rVert_2 \leq \sqrt{2d}$ while $\sup_{\mathbf{x}, \mathbf{y} \in \mathcal{X}} \lVert \mathbf{x} - \mathbf{y} \rVert_{\infty} \leq \sqrt{2}$. Thus, we can guarantee lower regret bound if we can describe regret bound with respect to $l_\infty$ norm instead of $l_2$ norm, especially when the dimension of the feasible set is high.

%%% Frechet derivative?
Considering norms other than $l_2$ norm poses a new challenge. While $\mathbf{x}_t \in \mathcal{X}$ is associated with norm $\lVert\ \cdot\ \rVert$, the value of gradient $\nabla f_t(\mathbf{x}_t)$, when considered as a Frech\'et derivative, is naturally associated with the dual norm $\lVert\ \cdot\ \rVert_{*}$. Hence, the equation $\mathbf{x}_t - \eta_t \nabla f_t(\mathbf{x}_t)$ requires extra caution. Moreover, what norm should we use for the projection operator? Can we use something other than a norm?

Mirror descent \cite{Yudin1983Mirror, Beck2003MirrorDA} surprisingly resolves all these questions. The key idea is to use a \textit{mirror map} to map the primal vector $\mathbf{x}_t$ to the dual space, and run gradient descent in the dual space. The mirror map will then map the resulting dual vector back to the primal space. We will mainly follow the exposition in \cite{Orabona2019OnlineLearning}.

We start by introducing some results from convex analysis. In this section, we will assume a technical condition that all mirror map $\psi$ is \textit{Legendre}.
\begin{defn}
A strictly convex $\mathcal{C}^1$ function $\psi : \mathcal{D} \subseteq \mathbb{R}^d \rightarrow \mathbb{R}$ is \textbf{Legendre} if $\lim_{\mathbf{x} \rightarrow \partial \mathcal{D}} \lVert \nabla \psi(\mathbf{x}) \rVert_* = +\infty$
\end{defn}

\begin{defn}
The Fenchel conjugate $f^{*}$ of a function $f : D \subseteq \mathbb{R}^d \rightarrow [-\infty, \infty]$ is defined as
\begin{equation*}
    f^{*}(\mathbf{\theta}) = \sup_{\mathbf{x} \in D} \langle \mathbf{\theta}, \mathbf{x} \rangle - f(\mathbf{x})
\end{equation*}
\end{defn}

\begin{prop} \label{prop:legendre-inverse}
For a Legendre function $\psi$, $\nabla \psi^{*} = (\nabla \psi)^{-1}$.
\end{prop}

\begin{defn}
A Bergman divergence associated with a Legendre function $\psi : \mathcal{D} \rightarrow \mathbb{R}$ is $B_\psi : \mathcal{D} \times \mathrm{int} \mathcal{D} \rightarrow \mathbb{R}$ defined as
\begin{center}
    $B_\psi(\mathbf{x}, \mathbf{y}) = \psi(\mathbf{x}) - \psi(\mathbf{y}) - \langle \nabla \psi(\mathbf{y}), \mathbf{x} - \mathbf{y} \rangle$.
\end{center}
\end{defn}

Since $\psi$ is strictly convex, $B_\psi(\mathbf{x}, \mathbf{y}) \geq 0$ for all $\mathbf{x} \in \mathcal{D}$ and $\mathbf{y} \in \mathrm{int} \mathcal{D}$, with equality if and only if $\mathbf{x} = \mathbf{y}$. Therefore, Bergman divergence can be seen as a "distance" measure between $\mathbf{x}$ and $\mathbf{y}$. 

\begin{defn}
Let $\psi$ be a Legendre function and $\mathcal{X} \subset \mathbb{R}^d$ be a non-empty convex compact set such that $\mathcal{X} \cap \mathrm{dom}\ \psi \neq \varnothing$. The Bergman projection onto $\mathcal{X}$ associated with $\psi$ is defined as 
\begin{center}
    $\displaystyle \Pi_{\mathcal{X}, \psi}(\mathbf{x}) = \argmin_{\mathbf{y} \in \mathcal{X}} B_\psi(\mathbf{y}, \mathbf{x})$
\end{center}
\end{defn}
We state some useful properties of Bergman divergence and Bergman projection.
\begin{prop} (Non-expansivity) \label{prop:non-expansivity}
Let $B_\psi$ be the Bergman divergence with respect to $\psi : \mathcal{D} \rightarrow \mathbb{R}$. For any $\mathbf{x} \in \mathcal{D}$ and $\mathbf{y} \in \mathrm{int} \mathcal{D}$,
\begin{equation*}
    B_\psi(\mathbf{x}, \Pi_{\mathcal{X}, \psi}(\mathbf{y})) \leq B_\psi(\mathbf{x}, \mathbf{y})
\end{equation*}
\end{prop}
\begin{prop} \label{prop:jacobi-bergman}
Let $B_{\psi}$ be the Bergman divergence with respect to $\psi : \mathcal{D} \rightarrow \mathbb{R}$. For any $\mathbf{x}, \mathbf{y} \in \mathrm{int} \mathcal{D}$ and $\mathbf{z} \in \mathcal{D}$, 
\begin{center}
$B_{\psi}(\mathbf{z}, \mathbf{x}) + B_{\psi}(\mathbf{x}, \mathbf{y}) - B_{\psi}(\mathbf{z}, \mathbf{y}) = \langle \nabla \psi(\mathbf{y}) - \nabla \psi(\mathbf{x}), \mathbf{z} - \mathbf{x} \rangle$.
\end{center}
\end{prop}
\begin{prop} \label{prop:bergman-strong-convex}
Suppose the mirror map $\psi$ is $\lambda$-strongly convex with respect to norm $\lVert \cdot \rVert$. Then, for all $\mathbf{x} \in \mathcal{D}$ and $\mathbf{y} \in \mathrm{int} \mathcal{D}$, 
\begin{center}
    $\displaystyle B_\psi(\mathbf{x}, \mathbf{y}) \geq \frac{\lambda}{2}\lVert \mathbf{x} - \mathbf{y} \rVert$.
\end{center}
\end{prop}

\begin{lem}[First-order optimality condition]
Let $f$ be a convex $\mathcal{C}^1$ function and $\mathcal{C}$ be a non-empty convex set contained in $\mathrm{int} (\mathrm{dom} f)$. Then, 
\begin{center}
    $\displaystyle \mathbf{x}^{*} = \argmin_{\mathbf{x} \in \mathcal{C}} f(\mathbf{x})$ if and only if $\langle \nabla f(\mathbf{x}^*), \mathbf{u} - \mathbf{x}^* \rangle \geq 0$ for all $\mathbf{u} \in \mathcal{C}$.
\end{center}
\end{lem}
% \begin{lem}
% Let $\psi : \mathcal{D} \rightarrow \mathbb{R}$ be a Legendre function and $\mathcal{X} \subset \mathbb{R}^d$ be a non-empty convex compact set such that $\mathcal{X} \cap \mathrm{dom} \psi \neq \varnothing$. Suppose $\mathbf{x}^{*} = \argmin_{\mathbf{x} \in \mathcal{D}} \psi(\mathbf{x})$ exists and lies in $\mathrm{int} \mathcal{D}$. 
% \end{lem}
Applying the lemma to $f = B_\psi(\cdot, \mathbf{x})$, where $\mathbf{x} \in \mathrm{int} \mathcal{D}$, we have the following condition.
\begin{prop} \label{prop:optimal}
Let $\mathcal{X} \subset \mathbb{R}^d$ be a non-empty convex compact set such that $\mathcal{X} \cap \mathrm{dom}\ \psi \neq \varnothing$.
For any $\mathbf{x} \in \mathrm{int} \mathcal{D}$ and $\mathbf{u} \in \mathcal{X}$, $\langle \nabla \psi(\mathbf{x}^{*}) - \nabla \psi(\mathbf{x}), \mathbf{u} - \mathbf{x^{*}} \rangle \geq 0$, where $\mathbf{x}^* = \argmin_{\mathbf{y} \in \mathcal{X}} B_\psi(\mathbf{y}, \mathbf{x})$.
\end{prop}
We now introduce the online mirror descent (OMD) algorithm. 
\begin{algorithm} 
\caption{Online Mirror Descent} \label{alg:omd}
\begin{algorithmic}[1]
\Require{Stopping time $T$, initial value $\mathbf{x}_1 \in \mathcal{X}$, step sizes $(\eta_t)_{t=1}^T$}
\For {$t=1 \dots T$}
\State Player makes decision $\mathbf{x}_t$
\State Player suffers $f_t(\mathbf{x}_t)$, and observes $f_t$
\State Update $ \mathbf{x}_{t+1} = \Pi_{\mathcal{X}, \psi}(\nabla \psi^{*}(\nabla \psi(\mathbf{x}_t) - \eta_t \nabla f_t(\mathbf{x}_t)))$
\EndFor
\end{algorithmic}
\end{algorithm}
\begin{rem}
Note in order for the algorithm to work, we need $\mathbf{x}_{t} \in \mathrm{int} \mathcal{X}$ because we should be able to compute $B_\psi(\mathbf{y}, \mathbf{x}_t)$ at each update step. $\psi$ being a Legendre function ensures that the minimum is not attained on the boundary.
\end{rem}

We immediately notice the resemblance between online gradient descent and online mirror descent. In fact, we can derive OGD from OMD, thereby showing that OGD is a special case of OMD. Consider OMD with mirror map $\psi(\mathbf{x}) = \frac{1}{2}\lVert \mathbf{x} \rVert_{2}^2$. Then, $\nabla \psi(\mathbf{x}) = \mathbf{x}$ and $B_\psi(\mathbf{x}, \mathbf{y}) = \frac{1}{2} \lVert \mathbf{x} - \mathbf{y} \rVert_2^2$, so line 4 in figure \ref{alg:omd} becomes 
\begin{center}
    $\displaystyle \mathbf{x}_{t+1} = \argmin_{\mathbf{y} \in \mathcal{X}} \lVert \mathbf{x}_t - \eta_t \nabla f_t(\mathbf{x}_t) \rVert_2^2$
\end{center}
which is equivalent to the update step in figure \ref{alg:ogd}. Therefore, we have shown that OMD is a generalization of OGD. 

We now prove regret bound of online mirror descent. Since online mirror descent generalizes online gradient descent, we expect a result similar to proposition \ref{prop:ogd-bound}. For that, we will assume that $\psi$ is $\lambda$-strongly convex with respect to norm $\lVert \cdot \rVert$ (note $\psi(\mathbf{x}) = \frac{1}{2}\lVert \mathbf{x} \rVert_2^2$ is $1$-strongly convex with respect to $l_2$ norm). 

We start by analyzing a single step of OMD.
\begin{lem} \label{lem:step-omd}
With notation in algorithm \ref{alg:omd}, the following inequality holds for all $\mathbf{u} \in \mathcal{X}$
\begin{center}
    $\displaystyle \eta_t(f_t(\mathbf{x}_t) - f_t(\mathbf{u})) \leq  B_\psi(\mathbf{u}, \mathbf{x}_t) - B_\psi(\mathbf{u}, \mathbf{x}_{t+1}) + \frac{\eta_t^2}{2\lambda}\lVert \nabla f_t(\mathbf{x}_t) \rVert_*^2$
\end{center}
\end{lem}
\begin{proof}
Since by assumption all loss function is convex, 
\begin{center}
    $\eta_t(f_t(\mathbf{x}_t) - f_t(\mathbf{u})) \leq \eta_t \langle \nabla f_t(\mathbf{x}_t), \mathbf{x}_t - \mathbf{u} \rangle$,
\end{center}
so it suffices to provide an upper bound of $\eta_t \langle \nabla f_t(\mathbf{x}_t), \mathbf{x}_t - \mathbf{u} \rangle$. This term can be decomposed into three terms,
\begin{align*}
    \eta_t \langle \nabla f_t(\mathbf{x}_t), \mathbf{x}_t - \mathbf{u} \rangle &= \langle \eta_t \nabla f_t(\mathbf{x}_t),\ \mathbf{x}_t - \mathbf{x}_{t+1} + \mathbf{x}_{t+1} - \mathbf{u} \rangle 
    \\
    &= \langle \eta_t \nabla f_t(\mathbf{x}_t) + \nabla \psi(\mathbf{x}_{t+1}) - \nabla \psi(\mathbf{x}_{t}) , \mathbf{x}_{t+1} - \mathbf{u} \rangle 
    \\ 
    &  + \langle \nabla \psi(\mathbf{x}_{t}) - \nabla \psi(\mathbf{x}_{t+1}), \mathbf{x}_{t+1} - \mathbf{u} \rangle 
    \\ 
    &  + \langle \eta_t \nabla f_t(\mathbf{x}_t), \mathbf{x}_t - \mathbf{x}_{t+1} \rangle
\end{align*}
Taking $\mathbf{x} = \nabla \psi^*(\nabla \psi(\mathbf{x}_t) - \eta_t \nabla f_t(\mathbf{x}_t))$ in proposition \ref{prop:optimal} and using proposition \ref{prop:legendre-inverse} we obtain that the first term is at most 0,
\begin{center}
    $\langle \nabla \psi(\mathbf{x_{t+1}}) - \nabla \psi(\mathbf{x}_t) + \eta_t \nabla f_t(\mathbf{x}_t), \mathbf{x}_{t+1} - \mathbf{u} \rangle \leq 0$
\end{center}
Using proposition \ref{prop:jacobi-bergman}, the second term can be written as
\begin{align*}
    \langle \nabla \psi(\mathbf{x}_t) - \nabla \psi(\mathbf{x}_{t+1}), \mathbf{x}_{t+1} - \mathbf{u} \rangle &= B_\psi(\mathbf{u}, \mathbf{x}_t) - B_\psi(\mathbf{u}, \mathbf{x}_{t+1}) - B_\psi(\mathbf{x}_{t+1}, \mathbf{x}_t)
    \\
    &\leq B_\psi(\mathbf{u}, \mathbf{x}_t) - B_\psi(\mathbf{u}, \mathbf{x}_{t+1}) - \frac{\lambda}{2} \lVert \mathbf{x}_t - \mathbf{x}_{t+1} \rVert^2
\end{align*}
where the inequality above follows from proposition \ref{prop:bergman-strong-convex}.
For final term, we use definition of dual norm (see \cite[Remark 4.18]{Orabona2019OnlineLearning}) and the AM-GM inequality
\begin{align*}
    \langle \eta_t \nabla_f(\mathbf{x}_t), \mathbf{x}_t - \mathbf{x}_{t+1} \rangle &\leq \lVert \eta_t \nabla f_t(\mathbf{x}_t) \rVert_* \lVert \mathbf{x}_t - \mathbf{x}_{t+1} \rVert 
    \\
    &\leq \frac{\lambda}{2} \lVert \mathbf{x}_t - \mathbf{x}_{t+1} \rVert^2 + \frac{\eta_t^2}{2\lambda}\lVert \nabla f_t(\mathbf{x}_t) \rVert_*^2
\end{align*}
In sum, we have the inequality
\begin{equation*}
    \eta_t\langle \nabla f_t(\mathbf{x}_t), \mathbf{x}_t - \mathbf{u} \rangle \leq B_\psi(\mathbf{u}, \mathbf{x}_t) - B_\psi(\mathbf{u}, \mathbf{x}_{t+1}) + \frac{\eta_t^2}{2\lambda}\lVert \nabla f_t(\mathbf{x}_t) \rVert_*^2 \qedhere
\end{equation*}
\end{proof}
\begin{thm} \label{prop:omd-bound}
Consider online mirror descent with constant step size $\eta$ using a mirror map $\psi$ $\lambda$-strongly convex with respect to $\lVert \cdot \rVert$. Take any starting point $\mathbf{x}_1 \in \mathcal{X}$ and let $\mathbf{x}^* \in \argmin_{\mathbf{x} \in \mathcal{X}} \sum_{t=1}^T f_t(\mathbf{x})$. Then we have the following regret bound
\begin{equation*}
    Reg_T^S \leq \frac{B_\psi(\mathbf{x}^*, \mathbf{x}_1)}{\eta} + \frac{\eta}{2\lambda} \sum_{t=1}^T \lVert \nabla f_t(\mathbf{x}_t) \rVert_*^2
\end{equation*}
Moreover, if $B_\psi$ is bounded above by some constant $D^2$ and $f_t$ is Lipschitz with constant $G$ for all $t = 1, \dots, T$, OMD attains sub-linear regret bound by choosing $\eta \propto \frac{1}{\sqrt{T}}$. If so, we attain regret bound of order $O(\sqrt{T})$
\end{thm}
\begin{proof}
Summing up the inequality in lemma \ref{lem:step-omd} for $t = 1, \dots, T$,
\begin{align*}
    \sum_{t=1}^T f_t(\mathbf{x}_t) - f_t(\mathbf{x}^*) &\leq \sum_{t=1}^T \left\{ \frac{1}{\eta}\left(B_\psi(\mathbf{x}^*, \mathbf{x}_t) - B_\psi(\mathbf{x}^*, \mathbf{x}_{t+1}) \right) + \frac{\eta}{2\lambda} \lVert \nabla f_t(\mathbf{x}_t) \rVert_*^2 \right\}
    \\
    &= \frac{1}{\eta} \left( B_\psi(\mathbf{x}^*, \mathbf{x}_1) - B_\psi(\mathbf{x}^*, \mathbf{x}_{T+1} \right) + \frac{\eta}{2\lambda} \sum_{t=1}^T \lVert \nabla f_t(\mathbf{x}_t) \rVert_*^2
    \\
    &\leq \frac{B_\psi(\mathbf{x}^*, \mathbf{x}_1)}{\eta} + \frac{\eta}{2\lambda} \sum_{t=1}^{T} \lVert \nabla f_t(\mathbf{x}_t) \rVert_*^2
\end{align*}
where the final inequality is due to non-negativity of Bergman divergence. If $B_\psi$ is bounded above by $D^2$ and $f_t$ is Lipschitz with constant $G$ for all $t = 1, \dots, T$,
\begin{align*}
    Reg_T^S \leq \frac{D^2}{\eta} + \frac{\eta TG^2}{2\lambda}
\end{align*}
Taking $\eta \propto \frac{1}{\sqrt{T}}$ gives the regret bound $Reg_T^S \leq \left(D^2 + \frac{G^2}{2\lambda}\right)\sqrt{T} = O(\sqrt{T})$.
\end{proof}
Note $\psi(\mathbf{x}) = \frac{1}{2} \lVert \mathbf{x} \rVert_2^2$ is $1$-strongly convex with respect to $l_2$ norm. Thus, we get theorem \ref{prop:ogd-bound} as an immediate corollary.

\begin{rem}
A crucial defect in our algorithm is its dependence on the time horizon $T$. That is, we need to know $T$ beforehand in order to get the regret bound $O(\sqrt{T})$ as the optimal step size $\eta$ depends on $T$. One solution to this is using "doubling trick" \cite{Auer1995GamblingIA} to let the algorithm run anytime. Basically, we run our algorithm using  $T=2^m$ on interval $T \in [2^m, 2^{m+1} - 1]$. Interested readers can consult \cite[Chapter2]{CesaBianchi2006PredictionLA}.    
\end{rem}


\section{Minimax Optimality} \label{sec:minimax}
Proposition \ref{alg:ogd} tells that for online convex optimization problem, OGD guarantees regret bound of order $O(\sqrt{T})$. A natural follow-up question is whether there exists an algorithm that guarantees lower regret bound. Surprisingly, it is known that $O(\sqrt{T})$ regret bound is optimal in case of online convex optimization. 

\begin{defn}[Minimax Regret] \label{def:minimax}
Consider online convex optimization problem with a feasible set $\mathcal{X}$ and loss function space $\mathcal{F}$.
The minimax regret of this online convex optimization problem is
\begin{center}
    $\displaystyle V_T(\mathcal{X}, \mathcal{F}) = \inf_{\pi} \sup_{f_1, \dots, f_T} \left( \sum_{t=1}^{T} f_t(\mathbf{x}_t) - \min_{\mathbf{x} \in \mathcal{X}} \sum_{t=1}^{T} f_t(\mathbf{x}) \right)$
\end{center}
where $\pi$ denote algorithm the player may use to choose their actions.
\end{defn}
Minimax regret tells the best possible regret against worst possible adversary. $\Omega(\sqrt{T})$ lower bound on the minimax regret implies that there is no algorithm that can guarantee better regret bound than of order $\Omega(\sqrt{T})$ for all online convex optimization problem.

The following proposition is inspired by \cite[Theorem~5.1]{Orabona2019OnlineLearning}, which proves the proposition in OGD ($l_2$ norm) setting. Here we extend to the result to arbitrary $l_p$ norm for $p \in [1, \infty)$. Note that the dual of $l_p$ norm is $l_q$ norm, where $q$ is H\"older conjugate of $p$.

\begin{prop}\label{prop:cvx-minimax}
Consider an online convex optimization problem with bounded feasible set $\mathcal{X} = \{\mathbf{x} \in \mathbb{R}^d : \lVert \mathbf{x} \rVert_p \leq D/2 \}$. Assume that the adversary chooses functions from $\mathcal{F} = \{ f(\mathbf{x}) = \langle \mathbf{x}, \mathbf{w} \rangle \mid \lVert \mathbf{w} \rVert_q \leq G \}$ for some $G > 0$. Then, the minimax regret satisfies 
\begin{center}
    $V_T(\mathcal{X}, \mathcal{F}) \geq \frac{GD}{2}\sqrt{T}$
\end{center}
\end{prop}
The main idea behind the proof, which will be relevant to every theorem regarding lower bound of minimax regret, is that we can lower bound $\sup_{f_1, \dots, f_T} Reg_T^S$, which is typically difficult to compute, by the expectation over random adversary. That is, we have
\begin{center}
    $\displaystyle \sup_{f_1, \dots, f_T} Reg_T^S \geq \sup_{D}\ \mathbb{E}_{(\mathbf{f}_t)_{t=1}^T \sim D} \left\{ \sum_{t=1}^T \mathbf{f}_t(\mathbf{x}_t) - \min_{\mathbf{x} \in \mathcal{X}} \sum_{t=1}^T \mathbf{f}_t(\mathbf{x}) \right\},$ 
\end{center}
where $D$ denotes the distribution of the sequence $(\mathbf{f}_t)_{t=1}^T$. Choosing appropriate $D$, computing the expectation becomes tractable. 
\begin{proof}
Following from the discussion above, we need to define an appropriate distribution $D$ which leads to minimax regret bound of order $\Omega(\sqrt{T})$.

First, fixed any points $\mathbf{u}, \mathbf{v} \in \mathcal{X}$ be such that $\lVert \mathbf{u} - \mathbf{v} \rVert_p = D$. Then, standard result in linear algebra states that there is $\mathbf{z} \in \mathbb{R}^d$ such that $\lVert \mathbf{z} \rVert_q = 1$ and $\langle \mathbf{z}, \mathbf{u} - \mathbf{v} \rangle = D$ (see \cite[Chapter 7]{Royden2010Real}). 

Since $\mathbf{f}_t$ is linear by assumption, we need to define the distribution of $(\mathbf{w}_t)_{t=1}^T$. Let $\epsilon_1, \dots, \epsilon_T$ be independent Rademacher random variables. Define $\mathbf{w}_t = G\epsilon_t \mathbf{z}$ for all $t \in [T]$. Then, we have
\begin{align*}
    \mathbb{E} \Big\{\sum_{t=1}^T \mathbf{f}_t(\mathbf{x}_t) - \min_{\mathbf{x} \in \mathcal{X}} \sum_{t=1}^T \mathbf{f}_t(\mathbf{x}) \Big\} 
    &= 
    \mathbb{E} \Big[\sum_{t=1}^T \langle \mathbf{x}_t, \mathbf{w}_t \rangle\Big] - \mathbb{E} \Big[\min_{\mathbf{x} \in \mathcal{X}} \sum_{t=1}^T \langle \mathbf{x}, \mathbf{w}_t \rangle \Big]
\end{align*}
We first tackle first term of RHS. Using law of total expectation (see \cite[Theorem 9.7]{Williams1991Probability}), 
\begin{align*}
    \mathbb{E}_{\mathbf{w}_1, \dots, \mathbf{w}_T} \Big[\sum_{t=1}^T \langle \mathbf{x}_t, \mathbf{w}_t \rangle \Big] 
    &= \mathbb{E}_{\mathbf{w}_{1}, \dots, \mathbf{w}_{T-1}} \mathbb{E}_{\mathbf{w}_T} \Big[\sum_{t=1}^T \langle \mathbf{x}_t, \mathbf{w}_t \rangle\ \Big|\ \mathbf{w}_1, \dots, \mathbf{w}_{T-1} \Big]
\end{align*}
Note $\mathbf{x}_T$, which is determined by player's algorithm $\pi$, is a function of losses prior to $T$. Hence, using that $\mathbf{w}_T$ is independent of other losses,
\begin{align*}
    \mathbb{E}_{\mathbf{w}_T} \Big[\langle \mathbf{x}_T, \mathbf{w}_T\rangle\ \Big|\ \mathbf{w}_1, \dots, \mathbf{w}_{T-1} \Big] &= \langle \mathbf{x}_T, \mathbb{E}_{\mathbf{w}_T} \mathbf{w}_T \rangle
    \\
    &= \langle \mathbf{x}_T,G[\mathbb{E} \epsilon_T] \mathbf{z} \rangle
    \\
    &= 0.
\end{align*}
Applying same argument for $t=T-1, \dots, 1$, we have
\begin{align*}
    \mathbb{E}_{\mathbf{w}_1, \dots, \mathbf{w}_T} \Big[\sum_{t=1}^T \langle\mathbf{x}_t, \mathbf{w}_t\rangle\Big] &= \mathbb{E}_{\mathbf{w}_1, \dots, \mathbf{w}_{T-1}} \Big[\sum_{t=1}^{T-1} \langle\mathbf{x}_t, \mathbf{w}_t\rangle\Big]
    = \dots = 0
\end{align*}
Now we analyze second term. We have
\begin{align*}
    -\mathbb{E} \min_{\mathbf{x} \in \mathcal{X}} \left\langle \mathbf{x}, \sum_{t=1}^T \mathbf{w}_t \right\rangle &= 
    \mathbb{E} \max_{\mathbf{x} \in \mathcal{X}} \left\langle \mathbf{x}, -\sum_{t=1}^T G\epsilon_t\mathbf{z} \right\rangle \nonumber
    \\
    &= \mathbb{E} \max_{\mathbf{x} \in \mathcal{X}} \left\langle \mathbf{x}, \sum_{t=1}^T G\epsilon_t \mathbf{z} \right\rangle \nonumber
    \\
    &\geq \mathbb{E} \max_{\mathbf{x} \in \{\mathbf{u}, \mathbf{v}\}} \left\langle \mathbf{x}, \sum_{t=1}^T G\epsilon_t\mathbf{z} \right\rangle \nonumber
    \\
    &= \mathbb{E} \left[\frac{1}{2}\sum_{t=1}^T L\epsilon_t \langle \mathbf{u} + \mathbf{v}, \mathbf{z} \rangle + \frac{1}{2}\ \middle| \sum_{t=1}^T G\epsilon_t \langle \mathbf{u} -\mathbf{v}, \mathbf{z} \rangle \middle|\ \right]
    \\
    &= \frac{GD}{2} \mathbb{E} \left[\middle| \sum_{t=1}^T \epsilon_t \middle| \right] \nonumber
\end{align*}
For second inequality, we use that $\epsilon_t$ and $-\epsilon_t$ have same distribution, and for last inequality, we use that $\mathbb{E} \epsilon_t = 0$ and $\langle \mathbf{u} -\mathbf{v}, \mathbf{z} \rangle$. Now the result follows from Khinchine's inequality.
\end{proof}
\begin{lem}[Khinchine's inequality]
Let $a_1, \dots, a_n$ be real numbers, and let $\epsilon_1, \dots, \epsilon_n$ be i.i.d. Rademacher random variables. Then,
\begin{center}
    $\displaystyle \mathbb{E}\left|\sum_{i=1}^n a_i\epsilon_i\right| \geq \frac{1}{\sqrt{2}}\sqrt{\sum_{i=1}^n a_i^2}$
\end{center}
\end{lem}
\begin{proof}
See \cite[Lemma~8.2]{CesaBianchi2006PredictionLA}
\end{proof}

Note we can easily extend the conclusion of the proposition to convex losses since every linear function is convex. 


\section{Prediction with Expert Advice}\label{section:expert-advice}
Expert problem is one of the most investigated classes of online learning problem. Here, the player makes a decision after witnessing advice from a number of experts, where the number of experts is finite. This problem is theoretically interesting in its own right, but also has many applications and particularly necessary for our main algorithm in \ref{Chap3}. The precise mathematical formulation of this problem is as follows \cite{CesaBianchi2006PredictionLA}:
\begin{defn}[Prediction from Expert Advice]
Let $\mathcal{X} \subset \mathbb{R}^d$ be a non-empty closed convex set of feasible actions and $(f_t)_{t \geq 0}$ be a sequence of loss functions. Let $n$ be the number of experts. 

On each round $t = 1, \dots T$,
\begin{itemize}
    \item Experts gives predictions $\mathbf{x}_t^{i} \in \mathcal{X}, i = 1, \dots, n$
    \item Player makes a decision $\mathbf{x}_t \in \mathcal{X}$ based on experts'
     predictions
    \item Player suffers loss $f_t(\mathbf{x}_t)$ and observes $\nabla f_t(\mathbf{x}_t)$
    \item Each expert $i$ suffers loss $f_t(\mathbf{x}_t^{i})$
\end{itemize}
The objective is to minimize regret with respect to the best expert in hindsight. That is, we minimize
\begin{equation*}
     Reg_T^S = \sum_{t=1}^T f_t(\mathbf{x}_t) - \min_{i \in [n]} \sum_{t=1}^T f_t(\mathbf{x}_t^{i}).
\end{equation*}
In prediction with expert advice setting, we will call the above equation as regret.
\end{defn}

Key challenge in prediction with expert advice is deciding how the player aggregates expert opinions. A natural strategy a player can take is taking a weighted sum of experts' predictions. That is, the player plays 
\begin{equation*}
    \mathbf{x}_t = \sum_{i=1}^n w_t^i \mathbf{x}_t^{i} \quad \text{s. t.} \quad \sum_{i=1}^n w_t^{i} = 1, w_t^{i} \geq 0\ \forall i \in [n]
\end{equation*}
where $w_t^i$ is the weight associated with expert $i$ at time $t$. Note we need $\mathcal{X}$ to be convex in order for the player's action to lie inside $\mathcal{X}$. The goal is to find an algorithm that can optimally adjust these weights at each time so that the player suffers the least regret. 

We can naturally put our strategy in the framework of online convex optimization as follows. Consider $n$ dimensional simplex, which is defined as
\begin{center}
    $\Delta^{n-1} = \left\{\mathbf{z} = (z_1, \dots, z_n) \in \mathbb{R}^n \mid z_i \geq 0\ \forall i \in [n], \lVert \mathbf{z} \rVert_1 = 1 \right\}$.
\end{center}
Then, $\Delta^{n-1}$ is the set of all possible weight vectors. For simplicity we will impose every loss function is linear and uniformly bounded on $\Delta^{n-1}$. That is, there is a constant $c > 0$ such that
\begin{center}
    $\mathcal{F} = \left\{ f(\mathbf{z}) = \langle \mathbf{z}, \mathbf{w} \rangle \mid \lVert \mathbf{w} \rVert_\infty \leq c \right\}$.  
\end{center}

With feasible set $\mathcal{X} = \Delta^{n-1}$ and loss function space $\mathcal{F}$ defined, we can now use online mirror descent to solve prediction with expert advice problem. Consider \textbf{negative entropy} function
\begin{center}
    $\psi(\mathbf{x}) = \sum_{i=1}^n x_i \log x_i \quad \text{where} \quad 0\log0 = 0$.
\end{center}

Negative entropy satisfied the following strong convexity property (see \cite[lemma 16]{ShalevShwartz2007OnlineLT}).
\begin{lem}
$\psi$ is $1$-strongly convex with respect to $l_1$ norm over the set $\Delta^{n-1}$.
\end{lem}

Additionally, note $\nabla \psi(\mathbf{x}) = \log\mathbf{x} + 1$ for $\mathbf{x} \in \mathbb{R}_{+}^n$, so $\lVert \nabla \psi(\mathbf{x})\rVert \rightarrow \infty$ as $\mathbf{x} \rightarrow \partial \mathbb{R}_{+}^n$. Thus $\psi$ is Legendre.  

The Bergman divergence of $\psi$ is
\begin{align*}
    B_\psi(\mathbf{x}, \mathbf{y}) &= \sum_{i=1}^n x_i \log \left(\frac{x_i}{y_i}\right),
\end{align*}
which is called the \textit{Kullback-Leiber (KL) divergence} when $\mathbf{x}, \mathbf{y} \in \mathcal{X}$ denote probability vectors. 

Using KKT conditions to solve constraint optimization problem \cite{Boyd2004ConvexO}, 
\begin{center}
    $\displaystyle \argmin_{\mathbf{x} \in \mathcal{X}} \sum_{i=1}^n x_i \log\left(\frac{x_i}{y_i}\right)$
\end{center}
for given $\mathbf{y} \in \mathbb{R}_{+}^n$, we obtain
\begin{center}
    $\displaystyle \Pi_{\mathcal{X}, \psi}(\mathbf{y}) = \frac{\mathbf{y}}{\lVert \mathbf{y} \rVert_1}$.
\end{center}
So the Bergman projection $\Pi_{\mathcal{X}, \psi}$ is a softmax function. Using proposition \ref{prop:legendre-inverse},
\begin{center}
    $\nabla \psi^*(\mathbf{x}) = (\nabla \psi)^{-1}(\mathbf{x}) = \exp(\mathbf{x} + 1)$
\end{center}

Now we are ready to derive a closed form of the update step.
\begin{align*}
    \Tilde{\mathbf{x}}_{t+1} &\coloneqq \nabla \psi^*(\nabla \psi(\mathbf{x}_t) - \eta_t\nabla f_t(\mathbf{x}_t))
    \\
    &= \exp( \log \mathbf{x}_t + 1 - \eta_t \nabla f_t(\mathbf{x}_t) - 1) = \exp(\log \mathbf{x}_t - \eta_t \nabla f_t(\mathbf{x}_t))
    \\
    &= \mathbf{x}_t \exp(-\eta_t \nabla f_t(\mathbf{x}_t))
    \\
    \mathbf{x}_{t+1, j} &= \frac{\Tilde{\mathbf{x}}_{t+1, j}}{\lVert \Tilde{\mathbf{x}}_{t+1} \rVert_1} \propto \mathbf{x}_{t, j} \exp(-\eta_t [\nabla f_t(\mathbf{x}_t)]_j)
\end{align*}

The final algorithm, described in figure \ref{alg:eg}, is called exponentiated gradient (EG) or multiplicative weights update, which is first discovered in \cite{Kivinen1997ExponentiatedGV}.
    
\begin{algorithm}
\caption{Exponentiated Gradient} \label{alg:eg}
\begin{algorithmic}
\Require{Stopping time $T$, initial value $\mathbf{x}_1 \in \mathrm{int} \Delta^{n-1}$, step size $\eta$}
\For{$t= 1, \dots T$}
\State Player plays $\mathbf{x}_t$
\State Player suffers $f_t(\mathbf{x}_t)$, and observes $f_t$
\State Update $\mathbf{x}_{t+1, j} \propto \mathbf{x}_{t, j}\exp(-\eta [\nabla f_t(\mathbf{x}_t)]_j)$ for all $j = 1, \dots, d$
\EndFor
\end{algorithmic}
\end{algorithm}

As often we fix the initial value $\mathbf{x}_1 = (1/n, \dots, 1/n) \in \Delta^{n-1}$ to be uniform across $n$. Then, we have the following bound for EG algorithm.
\begin{prop}\label{prop:eg-bound}
Exponentiated gradient algorithm with uniform initial value attains regret bound of order $O(\sqrt{2T \log(n)})$
\end{prop}
\begin{proof}
Let $\mathbf{x}_1 = (1/n, \dots, 1/n)$. Then for all $\mathbf{x} = (x_1, \dots, x_n) \in \Delta^{n-1}$,
\begin{align*}
    B_\psi(\mathbf{x}, \mathbf{x}_1) &= \sum_{t=1}^n x_i (\log x_i + \log n) = \psi(\mathbf{x}) + \log n
    \\
    &\leq \log n
\end{align*}
since $\psi \leq 0$ on $\Delta^{n-1}$.
Let $\mathbf{x}_* = \argmin_{i \in [n]} \sum_{t=1}^T f_t(\mathbf{x}_t^i)$.
Using theorem \ref{prop:omd-bound},
\begin{align*}
    \sum_{t=1}^T f_t(\mathbf{x}_t) - \min_{i \in [n]} \sum_{t=1}^T f_t(\mathbf{x}_t^i) &\leq \frac{B_\psi(\mathbf{x}_*, \mathbf{x}_1)}{\eta} + \frac{\eta}{2\lambda} \sum_{t=1}^T \lVert \nabla f_t(\mathbf{x}_t) \rVert_{\infty} 
    \\
    &\leq \frac{\log n}{\eta} + \frac{\eta Tc^2}{2}
\end{align*}
Note we use that $\psi$ is $1$-strongly convex with respect to $l_1$ norm. Taking $\eta = \sqrt{\frac{2 \log n}{Tc^2}}$, we obtain the optimal regret bound
\begin{equation*}
    \sum_{t=1}^T f_t(\mathbf{x}_t) - \min_{i \in [n]} \sum_{t=1}^T f_t(\mathbf{x}_t^i) \leq c\sqrt{2T\log n}  \qedhere
\end{equation*}
\end{proof}

We now prove lower bound for prediction with expert advice problem. We will show that asymptotically, our algorithm is the best possible. The idea behind the proof is similar to the that of proposition \ref{prop:cvx-minimax}; we will choose appropriate distribution on the weights $(\mathbf{w}_t)_{t=1}^T$ which will yield the result.
\begin{thm}\label{thm:expert-advice-lower-bound}
Consider a prediction with expert advice problem where the adversary chooses from $\mathcal{F} = \{f(\mathbf{x}) = \langle \mathbf{x}, \mathbf{w} \rangle \mid \lVert \mathbf{w} \rVert_{\infty} \leq 1 \}$. Then, 
\begin{equation*}
    \sup_{T, n} \frac{V_T(\Delta^{n-1}, \mathcal{F})}{\sqrt{2T \log n}} \geq 1
\end{equation*}
\end{thm}
\begin{proof}
Let $D$ be Bernoulli distribution with $p=1/2$. Suppose $\mathbf{w}_1, \dots, \mathbf{w}_T$ are sampled independently from $D^n$. 

Fix an algorithm $\pi$. Let $\mathbf{e}_i$ denote standard $i^{th}$ vector. Now,

\begin{align*}
    \mathbb{E} \left\{ \sum_{t=1}^T \mathbf{f}_t(\mathbf{x}_t) - \min_{i \in [n]} \sum_{t=1}^T \mathbf{f}_t(\mathbf{e}_i) \right\} &= \mathbb{E} \left\{\sum_{t=1}^T \langle \mathbf{x}_t, \mathbf{w}_t \rangle \right\} - \mathbb{E} \left\{\min_{i \in [n]} \sum_{t=1}^T \langle \mathbf{e}_i, \mathbf{w}_t \rangle \right\}
\end{align*}
We first analyze first term. Using the tower law of conditional expectation, for all $t = 1, \dots, T$,
\begin{align}
    \mathbb{E} [\langle \mathbf{x}_t, \mathbf{w}_t \rangle] &= 
    \mathbb{E} \Big[\mathbb{E} \Big[\langle \mathbf{x}_t, \mathbf{w}_t \rangle \Big]\ \Big|\ \mathbf{w}_1, \dots, \mathbf{w}_{t-1} \Big] \nonumber
    \\ 
    &= \mathbb{E} \Big\langle \mathbb{E} [\mathbf{w}_t], \mathbb{E}  \big[\mathbf{x}_t \big| \mathbf{w}_1, \dots, \mathbf{w}_{t-1} \big] \Big\rangle \nonumber
    \\
    &= \mathbb{E} \Big\langle \frac{1}{2}, \mathbb{E} \big[\mathbf{x}_t \big| \mathbf{w}_1, \dots, \mathbf{w}_{t-1} \big] \Big\rangle 
    \\
    &= \frac{1}{2}
\end{align}
where we use linearity of conditional expectation and independence of $(\mathbf{w}_t)_{t=1}^T$ in line (2.1) and that $\lVert \mathbf{x}_t \rVert_1 = 1$ in line (2.2). Therefore,  $\mathbb{E} \sum_{t=1}^T \langle \mathbf{x}_t, \mathbf{w}_t \rangle = T/2$. Now, 
\begin{align*}
    \mathbb{E} \left\{\sum_{t=1}^T \langle \mathbf{x}_t, \mathbf{w}_t \rangle \right\} - \mathbb{E} \left\{\min_{i \in [n]} \sum_{t=1}^T \langle \mathbf{e}_i, \mathbf{w}_t \rangle \right\} &= \frac{T}{2} - \mathbb{E} \left\{ \min_{i \in [n]} \sum_{t=1}^T \langle \mathbf{e}_i, \mathbf{w}_t \rangle \right\}
    \\
    &= \mathbb{E} \left\{ \max_{i \in [n]} \sum_{t=1}^T (1/2 - \langle \mathbf{e}_i, \mathbf{w}_t \rangle)\right\}
    \\
    &= \frac{1}{2} \mathbb{E} \left\{ \max_{i \in [n]} \sum_{t=1}^T \epsilon_{t, i} \right\}
\end{align*}
where $\epsilon_{t, i}$ is an independent Rademacher random variable. Using the lemma below, we prove the result we desire.
\end{proof}
\begin{lem}
Let $(\epsilon_{t, i})_{t=1, \dots, T, i = 1, \dots, n}$ be i.i.d. Rademacher random variables. Then,
We conclude with the following 
\begin{equation*}
    \lim_{T \rightarrow \infty}\lim_{n \rightarrow \infty} \frac{\mathbb{E} \left[ \max_{i \in [n]}\sum_{t=1}^T \epsilon_{t, i}\right]}{\sqrt{2T\log n}} = 1
\end{equation*}
\end{lem}
\begin{proof}
See \cite[lemma~A.22]{CesaBianchi2006PredictionLA}
\end{proof}

\chapter{Dynamic Regret}
\label{Chap3}
\section{Dynamic Regret}
So far the goal of online convex optimization is to design an algorithm that performs well against the single best decision in hindsight. That the comparison is against best \textit{offline} decision implies we compare against a \textit{static} environment. But often it is not realistic to assume stationary environment. If the underlying environment is changing, then the conventional regret is no longer a good measure of performance. 

To cope with these issues, we introduce a new objective called \textbf{dynamic regret}. 
\begin{defn}
Let $(\mathbf{x}_t)_{t=1}^{T}$ be decisions made by our algorithm. Let $(\mathbf{u}_t)_{t=1}^{T}$ be another sequence of feasible actions. Dynamic regret is defined as
\begin{center}
    $\displaystyle Reg_{T}^{D}(\mathbf{u}) = \sum_{t=1}^{T}f_t(\mathbf{x}_t) - \sum_{t=1}^{T}f_t(\mathbf{u}_t)$
\end{center}
where $f_t$ is a loss function at time $t=1, ..., T$. We call the sequence  $(\mathbf{u}_t)_{t=1}^T$ the comparator sequence.
\end{defn}
Dynamic regret can be understood as measuring regret with respect to a given sequence of actions. The non-stationary behavior can be modelled by the comparator sequence, making dynamic regret more reasonable objective than its static counterpart.

Dynamic regret naturally generalizes static regret by taking a constant comparator sequence $\mathbf{u}_t = \min_{\mathbf{x} \in \mathcal{X}} \sum_{t=1}^{T} f_t(\mathbf{x})$. It also generalizes \textbf{worst-case dynamic regret},
\begin{center}
    $\displaystyle Reg_{T}^{WD} = \sum_{t=1}^{T} f_t(\mathbf{x}_t) - \sum_{t=1}^{T} \min_{\mathbf{x} \in \mathcal{X}} f_t(\mathbf{x})$
\end{center}
by taking $\displaystyle \mathbf{u}_t \in \argmin_{\mathbf{x} \in \mathcal{X}} f_t(\mathbf{x})$. Note $\displaystyle Reg_T^{WD} = \max_{\mathbf{u}\in \mathcal{X}^T} Reg_T^D(\mathbf{u})$. 

% cite? 
In general, we can easily find a comparator sequence that prevents sub-linear regret if the player's actions are known a priori, so it is impossible to have an algorithm that attains sub-linear regret bound that is independent of the comparator.
Therefore, we relax our objective by letting the regret bound to be dependent on some measure of complexity of the comparator sequence. 

We focus on \textbf{path-length} variation of the comparator sequence
\begin{center}
    $\displaystyle P_T(\mathbf{u}) = \sum_{t=2}^{T} \lVert \mathbf{u}_t - \mathbf{u}_{t-1} \rVert$
\end{center}
with respect to a given norm $\lVert\ \cdot\ \rVert$. Path-length variation measures how much our comparator shifts during the time period $[1, T]$, hence a sensible measure of non-stationary behavior of the comparator. Indeed, we expect to recover static regret bound when $P_T(\mathbf{u}) = 0$. 

% For sake of completie briefly discuss an alternative approach to bound the regret . 
Another approach equally common in literature is letting regret bound depend on a measure of complexity of the adversarial sequence ($i.e.$ the loss function sequence $(f_t)_{t=1}^T$). 
For instance, \cite{Besbes2015Nonstationary, Jadbabaie2015OnlineO, Yang2016TrackingSM} each proposes algorithm that depends on functional variation of the loss function sequence
\begin{center}
    $\displaystyle V_T^f = \sum_{t=2}^T \sup_{\mathbf{x} \in \mathcal{X}} | f_t(\mathbf{x}) - f_{t-1}(\mathbf{x})|$,
\end{center}
while \cite{Chiang2013BeatingBI} considers gradient variation
\begin{center}
    $\displaystyle V_T^g = \sum_{t=2}^T \sup_{\mathbf{x} \in \mathcal{X}} \lVert \nabla f_t(\mathbf{x}) - \nabla f_{t-1}(\mathbf{x}) \rVert_{*}^2$.
\end{center}
These quantities depend only on the adversary's move, so do not measure non-stationary behavior of the comparator sequence. In literature, they are mostly used to bound worst-case dynamic regret. See \ref{table:metric} for table of summary of regret bounds.

It is worth noting that these metrics are not comparable in general, as path-length variation is a property of the comparator, while functional and gradient variation are property of the loss function sequence. But these metrics are not related to one another even if we take the comparator to be the minimizer of the loss function at each round. The following example is inspired by \cite{Jadbabaie2015OnlineO}.
\begin{exmp}
Consider a prediction with 2-expert problem. In this setting, player chooses a vector from 2-simplex and suffers linear loss $f_t(\mathbf{x}) = \langle \mathbf{x}, \mathbf{w}_t \rangle$ at each round. 

First, let $\mathbf{w}_t = (-\frac{1}{T}, 0)$ when $t$ is even and $\mathbf{w}_t = (0, -\frac{1}{T})$ when $t$ is odd. Clearly, $\mathbf{u}_t = (0, 1)$ when $t$ is even and $\mathbf{u}_t = (1, 0)$ otherwise. Also, $\lVert f_t - f_{t-1} \rVert_{\infty} = \frac{1}{T}$. Hence, we have 
\begin{align*}
    P_T(\mathbf{u}) &= \sum_{t=2}^T \lVert \mathbf{u}_t - \mathbf{u}_{t-1} \rVert_{1} = O(T) \\
    V_T^f &= \sum_{t=2}^T \lVert f_t - f_{t-1} \rVert_{\infty} = O(1)
\end{align*}

Now, let $\mathbf{w}_t = (-1, -\frac{1}{2}\ \cdot (1 + (-1)^t))$ for $t \geq 1$. We may make $\mathbf{u}$ to be a constant sequence that takes $(0, 1)$ for all $t \geq 1$. Note that $\lVert f_t - f_{t-1} \rVert_{\infty} = 1$. Therefore, in this case we have
\begin{align*}
    P_T(\mathbf{u}) &= \sum_{t=2}^T \lVert \mathbf{u}_t - \mathbf{u}_{t-1} \rVert_{1} = 0 \\
    V_T^f &= \sum_{t=2}^T \lVert f_t - f_{t-1} \rVert_{\infty} = O(T)
\end{align*}
\end{exmp}
\begin{center}
    \begin{table}[!h]
        \centering
        \small
        \begin{tabular}{|c|c|c|c|}
             \hline
             Feedback & Complexity measure & Regret Bound & Reference \\
             \hline
             \multirow{6}{*}{True Gradient} & $V_T^f$ & $O(T^{2/3}(1+V_T^f)^{2/3})$ & \cite{Besbes2015Nonstationary} \\
             \cline{2-4}
                                            & \multirow{2}{*}{$P_T(\mathbf{u})$} & $O(\sqrt{T}(1+P_T(\mathbf{u})))$ & \cite{Zinkevich2003OnlineCP} \\
             \cline{3-4}
                                            &                                    & $O(\sqrt{T(1 + P_T(\mathbf{u}))})$ & \cite{Zhang2018AdaptiveOL} \\
             \cline{2-4}
             & $P_T(\mathbf{u}), V_T^f$ & $O(\sqrt{(1 + P_T(\mathbf{u}) + V_T^f)(1+P_T(\mathbf{u}))})$ & \cite{Zhao2020DynamicRO} \\
             \cline{2-4}
                                            &  $V_T^f, V_T^g$ & 
             ${\begin{aligned}
             O(&\min \{(V_T^g + 1)^{1/2}T^{1/2},\\
             &(V_T^g + 1)^{1/3}T^{1/3}(V_T^f)^{1/3}\})    
             \end{aligned}}$ & \cite{Jadbabaie2015OnlineO} \\
             \hline
             Bandit & $P_T(\mathbf{u})$ & \begin{tabular}[c]{@{}l@{}}$O(\sqrt{T(1 + P_T(\mathbf{u}))})$ (two point) \\ $O(T^{3/4}(1+P_T(\mathbf{u}))^{1/4})$ (one point) \end{tabular} & \cite{Zhao2020BanditCO} \\
             \hline
        \end{tabular}
        \caption{Summary of dynamic regret bounds}
        \label{table:metric}
    \end{table}    
\end{center}


\begin{rem}[Adaptive Regret]
Another common generalization of static regret is \textbf{(strongly) adaptive regret} \cite{Hazan2007AdaptiveAF, Hazan2009EfficientLA, Daniely2015StronglyAO}, which is defined as the maximum of static regrets on each intervals $I = [r, s] \subseteq [T]$ of length $\tau$,
\begin{center}
    $\displaystyle Reg_{T}^{Ad}(\tau) = \sup_{\substack{I=[r, s] \subseteq [T] \\ |I|\ =\ \tau}} \left\{\sum_{t=r}^s f_t(\mathbf{x}_t) - \min_{\mathbf{x}_I \in \mathcal{X}} \sum_{t=r}^s f_t(\mathbf{x}_I) \right\}$
\end{center}
If the environment is non-stationary, different strategy will be effective on different interval $I \subseteq [T]$.
The intuition behind adaptive regret is that an algorithm effective against non-stationary environment must adapt well so that it performs well not only globally but also locally.

Understanding the relationship between dynamic regret and adaptive regret is an open question in online learning. For instance, the following proposition shows that we can use adaptive regret to bound dynamic regret.
\begin{prop} \cite[Theorem 3]{Zhang2018DynamicRO}
For all comparator sequence $\mathbf{u} \in \mathcal{X}^T$ and for all partition $\mathcal{P} = \{ \mathcal{I}_1, \dots, \mathcal{I}_k \mid k > 0 \}$ of $[T]$,
\begin{center}
    $\displaystyle Reg_T^D(\mathbf{u}) \leq \sum_{i=1}^{|\mathcal{P}|} \left\{Reg_T^{Ad}\left(|\mathcal{I}_i|\right) + 2|\mathcal{I}_i| \cdot V_T^f(\mathcal{I}_i) \right\}$
\end{center}
where $V_T^f(\mathcal{I}_i)$ is the functional variation of the loss sequence for interval $\mathcal{I}_i$. 
\end{prop}
\end{rem}

\section{Dynamic Regret for Online Convex Optimization}
We begin with understanding dynamic regret of general online convex optimization problem. Zinkevich \cite{Zinkevich2003OnlineCP} first introduced dynamic regret in online convex optimization setting. In this paper, he showed a simple baseline algorithm, OGD with constant step size, has a sub-linear regret bound. Following his idea, we prove an extension of his work that OMD with constant step size can also attain sub-linear regret bound.

In this section, we will assume the following:
\begin{enumerate}
    \item There is a constant $c > 0$ such that $0 \leq f(\mathbf{x})  \leq c$ for all $f \in \mathcal{F}$ and $\mathbf{x} \in \mathcal{X}$. \label{assumption:loss-bounded}
    \item $\mathcal{X}$ lies inside $\lVert \cdot \rVert$-ball of radius $D/2$.\label{assumption:feasible-bounded}
    \item There is a constant $G >0$ such that $\displaystyle \max_{\mathbf{x} \in \mathcal{X}} \lVert \nabla f(\mathbf{x}) \rVert_* \leq G$, where $\lVert \cdot \rVert_*$ is the dual norm of $\lVert \cdot \rVert$ in assumption \ref{assumption:feasible-bounded}. \label{assumption:loss-lipschitz}
    \item There is a constant $L$ such that $B_\psi(\mathbf{x}, \mathbf{y}) \leq L^2$ for all $\mathbf{x}, \mathbf{y} \in \mathcal{X}$ \label{assumption:bergman-bounded}
    \item Bergman divergence $B_\psi$ is \textit{Lipschitz} with respect to first argument and with respect to $\lVert \cdot \rVert$ norm in assumption \ref{assumption:feasible-bounded}. That is, there is a constant $M > 0$ such that
    \begin{center}
        $B_\psi(\mathbf{x}, \mathbf{z}) - B_\psi(\mathbf{y}, \mathbf{z}) \leq M \lVert \mathbf{x} -\mathbf{y} \rVert$
    \end{center} 
    for all $\mathbf{x}, \mathbf{y}, \mathbf{z} \in \mathcal{X}$.\label{assumption:bergman-lipschitz}
\end{enumerate}
Note assumption \ref{assumption:loss-lipschitz} implies that all loss functions are Lipschitz continuous. Assumptions \ref{assumption:bergman-bounded} and \ref{assumption:bergman-lipschitz} may seem artificial, but are crucial assumptions for proving the regret bound. Note for instance if $\psi$ is Lipschitz with respect to dual norm $\lVert \cdot \rVert_{*}$, then these assumptions hold. If $\lVert \nabla \psi(\mathbf{x}) \rVert_* \leq C$ for all $\mathbf{x} \in \mathcal{X}$, then 
\begin{align*}
    B_\psi(\mathbf{x}, \mathbf{z}) - B_\psi(\mathbf{y}, \mathbf{z}) &= \psi(\mathbf{x}) - \psi(\mathbf{y}) - \langle \nabla \psi(\mathbf{z}), \mathbf{x} -\mathbf{y} \rangle
    \\
    &\leq \langle \nabla \psi(\mathbf{x}) - \nabla\psi(\mathbf{z}), \mathbf{x} - \mathbf{y} \rangle
    \\
    &\leq \lVert \nabla \psi(\mathbf{x}) - \nabla\psi(\mathbf{z})\rVert_* \lVert \mathbf{x} - \mathbf{y} \rVert
\end{align*}
and the result following by using triangle inequality to bound $\lVert \nabla\psi(\mathbf{x}) - \nabla\psi(\mathbf{y}) \rVert_*$ by $2C$. OGD on bounded feasible domain satisfies this condition.
\begin{thm}
\label{thm:mirror-dynamic}
Consider online mirror descent with Bergman divergence $B_\psi$ defined by $\psi : \mathcal{X} \rightarrow \mathbb{R}$, $\lambda$-strongly convex with respect to $\lVert \cdot \rVert$.
Let the step sizes be constant throughout $t$, $i.e., \eta_t = \eta\ \forall t \in [T]$. Taking $\eta \propto \frac{1}{\sqrt{T}}$, online mirror descent guarantees regret bound
\begin{center}
    $Reg_{T}^{D}(\mathbf{u}) = O(\sqrt{T}(1 + P_T))$
\end{center}
\end{thm}
\begin{proof}
Continuing from lemma \ref{lem:step-omd}, we have
\begin{align*}
    \eta_t(f_t(\mathbf{x}_t) - f_t(\mathbf{u}_t)) &\leq B_\psi(\mathbf{u}_t, \mathbf{x}_t) - B_\psi(\mathbf{u}_t, \mathbf{x}_{t+1}) + \frac{\eta_t^2}{2\lambda} \lVert \nabla f_t(\mathbf{x}_t) \rVert_*^2
    \\ 
    &\leq B_\psi(\mathbf{u}_t, \mathbf{x}_t) - B_\psi(\mathbf{u}_{t+1}, \mathbf{x}_{t+1})
    \\
    &+ B_\psi(\mathbf{u}_{t+1}, \mathbf{x}_{t+1}) - B_\psi(\mathbf{u}_t, \mathbf{x}_{t+1})
    \\
    &+ \frac{\eta_t^2}{2\lambda} \lVert \nabla f_t(\mathbf{x}_t) \rVert_*^2
\end{align*}
Using Lipschitzness property of the Bergman divergence $B_\psi$, we can bound the term in the second line of RHS with respect to norm of the difference between the consecutive vectors of the comparator sequence
\begin{center}
    $B_\psi(\mathbf{u}_{t+1}, \mathbf{x}_{t+1}) - B_\psi(\mathbf{u}_t, \mathbf{x}_{t+1}) \leq M\lVert \mathbf{u}_{t+1} - \mathbf{u}_t \rVert$.
\end{center}

Letting $\eta_t = \eta$ and summing up the terms, we have
\begin{align*}
    \sum_{t=1}^T f_t(\mathbf{x}_t) - f_t(\mathbf{u}_t) &\leq \frac{1}{\eta} \sum_{t=1}^T \Big(B_\psi(\mathbf{u}_t, \mathbf{x}_t) - B_\psi(\mathbf{u}_{t+1}, \mathbf{x}_{t+1}) + M \lVert \mathbf{u}_{t+1} - \mathbf{u}_t \rVert \Big) 
    \\
    &+ \frac{\eta}{2\lambda} \sum_{t=1}^T \lVert \nabla f_t(\mathbf{x}_t) \rVert_*^2
    \\
    &= \frac{1}{\eta} \Big(B_\psi(\mathbf{u}_1, \mathbf{x}_1) - B_\psi(\mathbf{u}_{T+1}, \mathbf{x}_{T+1}) + MP_T(\mathbf{u})\Big) 
    \\
    &+ \frac{\eta}{2\lambda}\sum_{t=1}^T\lVert \nabla f_t(\mathbf{x}_t)\rVert_*^2
    \\
    &\leq \frac{L^2 + MP_T(\mathbf{u})}{\eta} + \frac{\eta TG^2}{2\lambda}
\end{align*}
% where we use assumptions for last inequality
Taking $\eta \propto \frac{1}{\sqrt{T}}$, we obtain $Reg_T^D(\mathbf{u}) = O(\sqrt{T}(1 + P_T))$
\end{proof}

\subsection{Lower Bound}
Now that we have a simple algorithm which achieves a sub-linear regret bound, it is again natural to question whether our algorithm is \textit{optimal}. To this end, we extend our definition \ref{def:minimax} of minimax regret so that it is comparable to dynamic regret.

\begin{defn}
Consider online convex optimization problem with feasible set $\mathcal{X}$ and loss function space $\mathcal{F}$. Let $\mathcal{C} \subseteq \mathcal{X}^T$ be a non-empty set of comparator sequences. The minimax dynamic regret with respect to $\mathcal{C}$ is defined as
\begin{center}
    $\displaystyle V_T^D(\mathcal{X}, \mathcal{F}, \mathcal{C}) = \inf_{\pi} \sup_{f_1, \dots, f_T} \sup_{\mathbf{u} \in \mathcal{C}} \left( \sum_{t=1}^T f_t(\mathbf{x}_t) - \sum_{t=1}^T f_t(\mathbf{u}_t) \right)$
\end{center}
where $\pi$ denote the algorithm the player may employ to choose their actions.
\end{defn}
Minimax dynamic regret shows the best possible dynamic regret against any adversary and any comparator sequence in $\mathcal{C}$. Minimax dynamic regret is not comparable to minimax regret defined in \ref{def:minimax} unless $\mathcal{C}$ contains the constant minimizer sequence $\mathbf{u}_t = \argmin_{\mathbf{x} \in \mathcal{X}} \sum_{t=1}^T f_t(\mathbf{x})$. 

\begin{thm} \label{thm:lower-bound-dynamic-ogd}
    For any online algorithm and any $\tau \in [0, TD]$, there exists a sequence of loss functions $(f_t)_{t=1}^{T}$ and comparators $(\mathbf{u}_t)_{t=1}^{T}$ satisfying assumptions 2 and 3 respectively, where
\begin{center}
    $P_T(\mathbf{u}_1, ..., \mathbf{u}_T) \leq \tau \quad \text{and} \quad Reg_T^{D} = \Omega(\sqrt{T(1 + \tau)})$
\end{center}
\end{thm}
\begin{proof}
Let $\mathcal{X} = \frac{D}{2}\mathbb{B}$ be a closed ball with radius $D/2$, and $\mathcal{F}$ be a set of convex functions that satisfies Lipschitz continuity assumption 3. 

For $\tau \in [0, TD]$, let $\mathcal{C}(\tau) = \{\mathbf{u} \in \mathcal{X}^T \mid P_T(\mathbf{u}) \leq \tau \}$ be the set of comparator sequences whose path-length is no more than $\tau$. Note the statement of the theorem can be rephrased as $V_T^D(\mathcal{X}, \mathcal{F}, \mathcal{C}(\tau)) = \Omega(G\sqrt{T(D^2 + D\tau)})$.  

First, assume $\tau < D$. Then, we can apply proposition \ref{prop:cvx-minimax} so that 
\begin{center}
    $\displaystyle V_T^D(\mathcal{X}, \mathcal{F}, \mathcal{C}(\tau)) \geq V_T(\mathcal{X}, \mathcal{F}) \geq \frac{GD}{2}\sqrt{T}$.
\end{center}
The proposition clearly follows from above as $\tau < D$.

Now, assume $\tau \geq D$. For simplicity, assume $M = \lceil \tau / D \rceil$ divides $T$, and let $L = T / M$.
Let $\mathcal{C}^\prime(\tau)$ be subset of $\mathcal{C}(\tau)$ which contains comparator sequences that can only change after each successive $L$ steps. Since $\mathcal{C}^\prime(\tau) \subseteq \mathcal{C}(\tau)$,
\begin{center}
    $\displaystyle V_T^D(\mathcal{X}, \mathcal{F}, \mathcal{C}(\tau)) \geq V_T^D(\mathcal{X}, \mathcal{F}, \mathcal{C}^\prime(\tau))$.
\end{center}
Hence it suffices to obtain lower bound of $V_T^D(\mathcal{X}, \mathcal{F}, \mathcal{C}^\prime(\tau))$. Exploiting the fact that $\mathbf{u} \in \mathcal{C}^\prime(\tau)$ can be decomposed into $\lceil \tau / D \rceil$ constant sequences each of length $L$, 

\begin{align*}
    V_T^D(\mathcal{X}, \mathcal{F}, \mathcal{C}^\prime(\tau)) &= \inf_{\pi} \sup_{f_1, \dots, f_T} \sup_{\mathbf{u} \in \mathcal{C}^\prime(\tau)} \left( \sum_{t=1}^T f_t(\mathbf{x}_t) - \sum_{t=1}^T f_t(\mathbf{u}_t) \right) \\
    &= \inf_{\pi} \sup_{f_1, \dots, f_T} \left( \sum_{t=1}^T f_t(\mathbf{x}_t) - \inf_{\mathbf{u} \in \mathcal{C}^\prime(\tau)} \sum_{t=1}^T f_t(\mathbf{u}_t) \right) \\
    &= \inf_{\pi} \sup_{f_1, \dots, f_T} \sum_{i=1}^M \left( \sum_{t=1 + (i - 1)L}^{iL} f_t(\mathbf{x}_t) - \inf_{x \in \mathcal{X}} \sum_{t=1 + (i - 1)L}^{iL} f_t(\mathbf{x})\right)
    \\
    &\geq M \cdot\ \inf_{\pi} \sup_{f_1, \dots, f_L} \left( \sum_{t=1}^{L} f_t(\mathbf{x}_t) - \inf_{\mathbf{x} \in \mathcal{X}} \sum_{t=1}^{L} f_t(\mathbf{x}) \right)
    \\
    &\geq \lceil \tau / D \rceil \frac{GD}{2} \sqrt{L} = \frac{GD}{2} \sqrt{T \lceil \tau / D \rceil} \geq \frac{G}{2}\sqrt{TD\tau},
\end{align*}
where we once again use proposition \ref{prop:cvx-minimax} in the second inequality. Now, since $\tau \geq D$,
\begin{equation*}
    V_T^D(\mathcal{X}, \mathcal{F}, \mathcal{C}(\tau)) \geq \frac{G}{2} \sqrt{TD\tau} = \Omega(\sqrt{T(1 + \tau)}) \qedhere
\end{equation*}
\end{proof}

\subsection{Minimax Optimal Algorithm}
From the proof of theorem \ref{thm:mirror-dynamic}, note we obtain regret bound 
\begin{center}
    $\displaystyle Reg_{T}^{D}(\mathbf{u}) \leq \frac{L^2 + MP_T(\mathbf{u})}{\eta} + \frac{\eta TG^2}{2\lambda}$.
\end{center}
Notice that to get the optimal bound, we need to set $\eta = \sqrt{\frac{2\lambda(L^2 + MP_T)}{TG^2}}$, whereby we attain optimal regret bound of order $Reg_{T}^{D} = O(\sqrt{(1+P_T)T})$ that matches the lower bound.
However, since the optimal step size now depends on the path length, the bound depends on specific choice of comparator sequence. Thus, the optimal step size is not obtainable, since we do not know the comparator sequence a priori. 

Zhang et. al. \cite{Zhang2018DynamicRO} first introduced an algorithm, based on  \cite{Erven2021MetaGradAU}, that guarantees regret bound of order $O(\sqrt{T(1 + P_T)})$ without using any specific information about possible comparator sequence. For the rest of this section, we follow \cite{Zhang2018DynamicRO}, but give a more generalized version so that it fits into our narrative more closely.

The main idea of their work is to run multiple OGD algorithms, each with its own (constant) step size. By carefully choosing the step sizes, we have at least one OGD algorithm that runs near optimally. The decisions of OGD algorithms are then aggregated by the controller algorithm, which we expect to put more weight on the decisions made by OGD that runs with the best step size. 

It is clear from the description of the main idea that the controller algorithm is essentially solving a \textbf{problem with expert advice} problem. Therefore, the controller algorithm takes the form of exponentiated gradient algorithm, as it is minimax optimal \footnote{In the original paper, master algorithm uses exponentially weighted average forecaster \cite{CesaBianchi2006PredictionLA}, where the player uses the value of the function instead of the gradient (as in exponentiated gradient), to update the weights. We note exponentially weighted average forecaster and exponentiated gradient has regret bound of same order.}.

\begin{algorithm}
\caption{Controller algorithm}\label{ader:controller}
\begin{algorithmic}[1]
\Require {A step size $\alpha$, a set $\mathcal{H}$ containing step sizes for workers, initial weight $\mathbf{w}_1 \in \Delta^{|\mathcal{H}|-1}$}
\For {$t = 1, \dots, T$}
\State Run a set of worker algorithm $\{E^{\eta} \mid \eta \in \mathcal{H} \}$ each with step size $\eta$ 
\State Receive $\mathbf{x}_t^{\eta}$ from each worker $E^{\eta}$
\State Output $\displaystyle \mathbf{x}_t = \sum_{\eta \in \mathcal{H}} w_{t}^{\eta} \mathbf{x}_t^{\eta}$
\State Observe the loss function $f_t$
\State Update the weight of each worker by \[
    w_{t+1}^{\eta} = \frac{w_t^{\eta}e^{-\alpha \nabla f_t(\mathbf{x}_t^{\eta})}}{\sum_{\nu \in \mathcal{H}}w_t^{\nu}e^{-\alpha \nabla f_t(\mathbf{x}_t^{\nu})}}
\]
\State Send gradient $\nabla f_t(\mathbf{x}_t^{\eta})$ each worker $E^{\eta}$ 
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{lem} \label{lem:controller}
The regret of controller algorithm \ref{ader:controller} satisfies
\begin{equation*} 
    \sum_{t=1}^T f_t(\mathbf{x}_t) - \sum_{t=1}^T f_t(\mathbf{x}_t^\eta) \leq \frac{\alpha Tc^2}{2} - \frac{\log w_{1, \eta}}{\alpha}
\end{equation*}
for all $\eta \in \mathcal{H}$.
\end{lem}
\begin{proof}
Note controller algorithm is a version of exponentiated gradient algorithm. Slight modification of the proof of proposition \ref{prop:eg-bound} implies
\begin{equation*}
    \sum_{t=1}^T f_t(\mathbf{x}_t) - \sum_{t=1}^T f_t(\mathbf{x}_t^\eta) \leq \frac{B_\psi(\mathbf{e}_\eta, \mathbf{w}_1)}{\alpha} + \frac{\alpha Tc^2}{2}
\end{equation*}
where $\mathbf{e}_\eta \in \Delta^{|\mathcal{H}| - 1}$ is a vector that is $1$ at index $\eta$ and $0$ elsewhere. We conclude by noting that $B_\psi(\mathbf{e}_\eta, \mathbf{w}_1) = -\log w_{1, \eta}$.
\end{proof}
Assuming that we have no knowledge of $\mathbf{x}_1$, the optimal $\alpha$ that minimizes the regret bound is $\alpha^* = \sqrt{\frac{2}{Tc^2}}$, which leads to regret bound
\begin{center}
    $\displaystyle \sum_{t=1}^T f_t(\mathbf{x}_t) - \sum_{t=1}^T f_t(\mathbf{x}_t^\eta) \leq \frac{\sqrt{2Tc^2}}{2}(1 - \log w_{1, \eta})$
\end{center}
Now we go onto the main theorem. For simplicity, we assume that $L = M = D$ and $\psi$ is $1$-strongly convex.
\begin{prop} \label{ader:bound}
Consider online convex optimization problem satisfying assumptions \ref{assumption:loss-bounded}-\ref{assumption:loss-lipschitz}, with $D = M = L$ and mirror map $\psi$ is $1$-strongly convex. Let 
\begin{equation*}
    \mathcal{H} = \left\{ \eta_i = \frac{2^{i-1}D}{G}\sqrt{\frac{2}{T}} \mid i = 1, \dots, N \right\}
\end{equation*}
where $N = \lceil \frac{1}{2} \log_2(T+1) \rceil + 1$. For any comparator sequence $\mathbf{u} \in \mathcal{X}^T$, algorithm \ref{ader:controller} with online mirror descent that satisfies assumptions \ref{assumption:bergman-bounded} and \ref{assumption:bergman-lipschitz} as worker algorithm guarantees regret bound
\begin{align*}
    \sum_{t=1}^T f_t(\mathbf{x}_t) - \sum_{t=1}^T f_t(\mathbf{u}_t) \leq \frac{3}{2}\sqrt{2(D^2 + DP_T)TG^2} + \frac{\sqrt{2Tc^2}}{2}(1 - \log x_{1, \eta_k}) 
\end{align*}
for some $k \in \{1, \dots, N \}$
\end{prop}
\begin{proof}
Recall from proposition \ref{thm:mirror-dynamic} that online mirror descent with constant step size $\eta$ attains regret bound of the form
\begin{equation*}
    \sum_{t=1}^T f_t(\mathbf{x}_t) - f_t(\mathbf{u}_t) \leq \frac{D^2 + DP_T}{\eta} + \frac{\eta TG^2}{2}
\end{equation*}
and we attain optimal regret bound if the step size is $\eta^* = \sqrt{\frac{2(D^2 + DP_T)}{TG^2}}$. 
Since our feasible set has diameter $D$, $0 \leq P_T(\mathbf{u}) \leq TD$ for any comparator $\mathbf{u}$. Therefore, optimal step size $\eta^*$ must lie inside the interval
\begin{equation*}
    \frac{D}{G}\sqrt{\frac{2}{T}} \leq \eta^* \leq \frac{D}{G}\sqrt{2\left(1 + \frac{1}{T}\right)}
\end{equation*}

Note we have set $\mathcal{H}$ so that $\min \mathcal{H} = \frac{D}{G}\sqrt{\frac{2}{T}}$ and $\max \mathcal{H} \geq \frac{D}{G}\sqrt{2 \left(1 + \frac{1}{T}\right)}$. Therefore, by construction there is $k$ such that $\eta_k \leq \eta^* \leq \eta_{k+1} = 2\eta_k$. Then, the regret bound of worker $E^{\eta_k}$ satisfies
\begin{align*}
    \sum_{t=1}^T f_t(\mathbf{x}_t^{\eta_k}) - f_t(\mathbf{u}_t) &\leq \frac{D^2 + DP_T}{\eta_k} + \frac{\eta_k TG^2}{2}
    \\
    &\leq \frac{2(D^2 + DP_T)}{\eta^*} + \frac{\eta^* TG^2}{2}
    \\
    &\leq \frac{3}{2}\sqrt{2(D^2 + DP_T)TG^2}
\end{align*}
Together with regret bound in lemma \ref{ader:controller}, we obtain 
\begin{equation*}
    \sum_{t=1}^T f_t(\mathbf{x}_t) - \sum_{t=1}^T f_t(\mathbf{u}_t) \leq \frac{3}{2}\sqrt{2(D^2 + DP_T)TG^2} + \frac{\sqrt{2Tc^2}}{2}(1 - \log w_{1, \eta_k}) \qedhere
\end{equation*}
\end{proof}

The regret bound in proposition \ref{ader:bound} depends on how to setup initial weight on the workers' outputs. Indeed, we do not know a priori which $k$ will satisfy $\eta_k \leq \eta^* \leq \eta_{k+1}$, since knowing $k = \lfloor \frac{1}{2} \log_2 (1+ \frac{P_T}{D}) \rfloor + 1$ implies knowing $P_T$ a priori. 

We can simply put uniform weight on all workers, $i.e., \mathbf{w}_1 = (1/|\mathcal{H}|, \dots, 1 / |\mathcal{H}|)$. However, since $|\mathcal{H}| = O(\log T)$, the regret bound matches the lower bound $\Omega(\sqrt{(1+P_T)T})$ only up to neglecting logarithmic factor $\log \log T$.

One way to get a bound without $\log \log T$ is to set $w_{1, \eta_k}$ inversely proportional to $k$. This will allow $\log w_{1, \eta_k}$ to be function of $k$, which will end up to be a term of order $\sqrt{T}\log P_T$, which is less than $\sqrt{T P_T}$. For instance, we can set $w_{1, \eta_k} = \frac{C}{k(k+1)}$, where $C = 1 - \frac{1}{|\mathcal{H}|+1}$ as suggested in \cite{Zhang2018AdaptiveOL}. Since $w_{1, \eta_k} \geq \frac{1}{k(k+1)} \geq 1/k^2$, we get a regret bound
\begin{equation*}
    Reg_T^D \leq \frac{3}{2}\sqrt{2(D^2 + DP_T)TG^2} + \frac{\sqrt{2Tc^2}}{2}(1 + 2 \log k) = O(\sqrt{T(1 + P_T)})
\end{equation*}


\section{Tracking the Best Expert}
We now turn our attention to prediction with expert advice problem. Previously, the player's decision is compared against decisions made by best expert. Now, we add additional dimension to the problem by comparing to a strategy that can switch a number of times which expert to take advice from. These problems occur naturally in practice; for instance, we would like to set up a recommendation strategy that can adapt to any preference change of customers. 
% In this section, we will not restrict to the case where the feasible set $\mathcal{X}$ of the problem is convex and hence introduce randomness to the strategy.

In this scenario, dynamic regret is often called as tracking regret, since the goal of the algorithm is to track the expert that locally yields best decision at each round $t$, and often prediction with expert advice problem with non-constant comparator is called \textbf{tracking the best expert problem}. Mathematically speaking, for given time horizon $T$ and number of experts $n$, dynamic regret of prediction with expert advice problem is given as
\begin{equation*}
    Reg_D^T(\mathbf{u}) = \sum_{t=1}^T f_t(\mathbf{x}_t) - \sum_{t=1}^T f_t(\mathbf{u}_t)
\end{equation*}
where $\mathbf{x}_t, \mathbf{u}_t \in \{1, \dots, n\}$. In this case, total number of switches
\begin{center}
    $S(\mathbf{u}) = |\{t \in [2, T] \mid \mathbf{u}_t \neq \mathbf{u}_{t-1}\}|$
\end{center} 
will play the role of path length variation.  

\subsection{Lower Bound}
We first prove lower bound of tracking regret. The main idea of the proof is the same as that of theorem \ref{thm:lower-bound-dynamic-ogd}. Given $S(\mathbf{u}) = m$, we can have a set of intervals in which the comparator stays constant. On each interval, we can use theorem \ref{thm:expert-advice-lower-bound} to establish lower bound.
\begin{prop}\label{prop:pred-dynamic-lower-bound}
The minimax dynamic regret of prediction with expert problem 
\begin{equation*}
    V_T^D(m) = \inf_\pi \sup_{f_1, \dots, f_T} \sup_{S(\mathbf{u}) \leq m} \sum_{t=1}^T f_t(\mathbf{x}_t) - \sum_{t=1}^T f_t(\mathbf{u}_t)
\end{equation*}
has asymptotic lower bound of order $\Omega(\sqrt{m T \log n})$.
\end{prop}
\begin{proof}
Without loss of generality, we may assume $m$ divides $T$. Let $L = T / m$. Consider $\mathcal{C}$ be the set of comparators that changes only after each $L$ steps. Then every comparator in $\mathcal{C}$ has at most $m$ number of switches. Therefore, for any algorithm $\pi$ and any sequence of losses $(f_t)_{t=1}^T$, 
\begin{equation*}
    \sup_{S(\mathbf{u}) \leq m} Reg_T^D(\mathbf{u}) \geq \sup_{\mathbf{u} \in \mathcal{C}} Reg_T^D(\mathbf{u}).
\end{equation*}
As every element $\mathbf{u} \in \mathcal{C}$ can be decomposed into $m$ constant sequences each of length $L$,
\begin{align*}
    \sup_{\mathbf{u} \in \mathcal{C}} Reg_T^D(\mathbf{u}) &= \sum_{t=1}^T f_t(\mathbf{x}_t) - \inf_{\mathbf{u} \in \mathcal{C}} \sum_{t=1}^T f_t(\mathbf{u}_t)
    \\
    &= \sum_{k=1}^m \left(\sum_{i = 1 + (k-1)L}^{k L} f_t(\mathbf{x}_t) - \inf_{x \in [n]} \sum_{i=1 + (k-1)L}^{kL} f_t(x) \right)
    \\
    &\geq m \left(\sum_{t=1}^L f_t(\mathbf{x}_t) - \inf_{x \in [n]} \sum_{t=1}^L f_t(x) \right)
    \\
    &\geq m \sqrt{2L \log n} 
    \\
    &= \sqrt{2 mT \log n} = \Omega(m T \log n) \qedhere
\end{align*}
\end{proof}

We would like to use theorem \ref{thm:mirror-dynamic} to exponentiated gradient, but we see that the theorem is not fully applicable. The problem is that there is no constant $M$ such that $B_\psi(\mathbf{u}_{t+1}, \mathbf{x}_{t+1}) - B_\psi(\mathbf{u}_t, \mathbf{x}_{t+1}) \leq M \lVert \mathbf{u}_{t+1} - \mathbf{u}_t \rVert_1$ for all $\mathbf{x}_{t+1} \in \mathrm{int} \Delta^{n-1}$. In particular, the problem happens because $\mathbf{x}_{t+1}$ can be arbitrary close to the boundary of $\Delta^{n-1}$. Thus, we need modify exponentiated gradient so that weight vector stays away from the boundary (see figure \ref{alg:modified-eg}).

One way to achieve this is to project the result from multiplicative update on the convex set $\Delta_\alpha^{n-1} = [\alpha/n, 1]^{n} \cap \Delta^{n-1}$ using Bergman projection $\Pi_{\Delta_\alpha^{n-1}, \psi}$ for some $\alpha \in (0, 1)$.  This was first introduced in \cite{Herbster2001Tracking} in their study of shifting regret in prediction with expert advice problem. We prove the regret bound of this method. As in section \ref{section:expert-advice}, we assume linear loss, and for simplicity, let the value of all loss function lie in the interval $[0, 1]$.

The proof of this proposition is inspired by \cite{CesaBianchi2012MirrorDM}. We begin with a lemma, which is a special case of lemma \ref{lem:step-omd}.
\begin{lem} \label{lem:exp-modified-step}
Consider exponentiated gradient followed by taking Bergman projection onto $\Delta_\alpha^{n-1}$ for some $\alpha > 0$. Then, for any $\mathbf{u}_t \in \Delta_\alpha^{n-1}$
\begin{equation*}
    \eta(f_t(\mathbf{x}_t) - f_t(\mathbf{v}_t)) \leq B_\psi(\mathbf{v}_t, \mathbf{x}_t) - B_\psi(\mathbf{v}_t, \mathbf{x}_{t+1}) + \frac{\eta^2}{2}
\end{equation*}
\end{lem}
\begin{proof}
Note by non-expansivity of Bergman projection, 
$B_\psi(\mathbf{v}_t, \Tilde{\mathbf{x}}_{t+1}) \geq B_\psi(\mathbf{v}_t, \mathbf{x}_{t+1})$. Then, using lemma \ref{lem:step-omd}, we have
\begin{align*}
    \eta(f_t(\mathbf{x}_t) - f_t(\mathbf{v}_t)) &\leq B_\psi(\mathbf{v}_t, \mathbf{x}_t) - B_\psi(\mathbf{v}_t, \Tilde{\mathbf{x}}_{t+1}) + \frac{\eta^2}{2}
    \\
    &\leq B_\psi(\mathbf{v}_t, \mathbf{x}_t) - B_\psi(\mathbf{v}_t, \mathbf{x}_{t+1}) + \frac{\eta^2}{2} \qedhere
\end{align*} 
\end{proof}

\begin{prop} \label{prop:proj-eg-bound}
Exponentiated gradient followed by taking Bergman projection onto $\Delta_\alpha^{n-1}$ attains regret bound
\begin{equation*}
    \sum_{t=1}^T f_t(\mathbf{x}_t) - f_t(\mathbf{u}_t) \leq \frac{\log n + 4\log(n/\alpha) S}{\eta} + \left(\frac{\eta}{2} + \alpha\right)T
\end{equation*}
for any comparator sequence $\mathbf{u}_1, \dots, \mathbf{u}_T \in \Delta^{n-1}$.
\end{prop}
\begin{proof}
First notice that for $\mathbf{x} \in \Delta_\alpha^{n-1}$, 
\begin{center}
    $\displaystyle \lVert \nabla \psi(\mathbf{x}) \rVert_\infty = \max_{i=1, \dots, n} (\log x_i - 1) \leq \log\left(\frac{n}{\alpha}\right)$.
\end{center}
Therefore, from the discussion of assumption \ref{assumption:bergman-lipschitz}, the Bergman divergence $B_\psi$ is Lipschitz with respect to $l_1$ norm with constant $2\log(n/\alpha)$.

Now, for $t = 1, \dots, T$, define $\mathbf{v}_t = (1 - \alpha)\mathbf{u}_t + (\alpha/n) \mathbf{1}$, where $\mathbf{1} = (1, \dots, 1)$ is a vector with $1$s on every entry. Notice that $\mathbf{v}_t$ lies in $\Delta_\alpha^{n-1}$. Theorem \ref{thm:mirror-dynamic} cannot be applied directly on $\mathbf{v} = (\mathbf{v}_1, \dots, \mathbf{v}_T)$, but by summing up bound on lemma \ref{lem:exp-modified-step}, we obtain

\begin{align}
    \sum_{t=1}^T f_t(\mathbf{x}_t) - f_t(\mathbf{v}_t) &\leq  \frac{1}{\eta} \left(B_\psi(\mathbf{u}_1, \mathbf{x}_1) + 2\log\left(\frac{n}{\alpha}\right)P_T(\mathbf{v}) \right) + \frac{\eta T}{2} \nonumber
    \\
    &= \frac{\log n + 2(1-\alpha)\log(n / \alpha) P_T(\mathbf{u})}{\eta} + \frac{\eta T}{2} \nonumber
\end{align}
where we use that $\mathbf{x}_1 = (1/n, \dots, 1/n)$ and that $P_T(\mathbf{v}) = (1 - \alpha)P_T(\mathbf{u})$ in the last equality. Note since $\mathbf{u}_t$ is a one-hot vector, $P_T(\mathbf{u}) = \sum_{t=2}^T \lVert \mathbf{u}_t - \mathbf{u}_{t-1} \rVert_1 = S/2$, where $S$ is the number of switches. Now, writing $f_t(\mathbf{x}) = \langle \mathbf{x}, \mathbf{w}_t \rangle$, we have
\begin{align*}
    f_t(\mathbf{x}_t) - f_t(\mathbf{u}_t) &= \langle \mathbf{x}_t - \mathbf{u}_t, \mathbf{w}_t \rangle
    \\
    &= \langle \mathbf{x}_t - \mathbf{v}_t, \mathbf{w}_t \rangle + \langle (\alpha/n) \mathbf{1} - \alpha \mathbf{u}_t, \mathbf{w}_t \rangle
\end{align*}
Since loss is assumed to be positive and $\lVert \mathbf{w}_t \rVert_\infty$ is bounded by $1$,
\begin{align*}
    \langle (\alpha/n)\mathbf{1} - \alpha \mathbf{u}_t, \mathbf{w}_t \rangle &= (\alpha/n) \langle \mathbf{1}, \mathbf{w}_t \rangle - \alpha \langle \mathbf{u}_t, \mathbf{w}_t \rangle 
    \\
    &\leq \alpha \lVert \mathbf{1} \rVert_1 \lVert \mathbf{w}_t \rVert_\infty = \alpha
\end{align*}
Therefore, summing up individual $f_t(\mathbf{x}_t) - f_t(\mathbf{u}_t)$ terms,
\begin{align*}
    \sum_{t=1}^T f_t(\mathbf{x}_t) - f_t(\mathbf{u}_t) &\leq \sum_{t=1}^T \Big[ f_t(\mathbf{x}_t) - f_t(\mathbf{v}_t) + \alpha \Big]
    \\ 
    &\leq \frac{\log n + 4(1 - \alpha) \log(n/\alpha) S}{\eta} + \left(\frac{\eta}{2} + \alpha\right)T 
    \\
    &\leq \frac{\log n + 4\log(n/\alpha) S}{\eta} + \left(\frac{\eta}{2} + \alpha\right)T\qedhere
\end{align*}
\end{proof}
There is two problems with this algorithm. First, the $O(\sqrt{TS\log n})$ bound is not attainable using proposition \ref{prop:pred-dynamic-lower-bound}. Given that $\alpha$ is fixed, optimal bound is 
\begin{equation*}
    \sum_{t=1}^T f_t(\mathbf{x}_t) - f_t(\mathbf{u}_t) \leq \sqrt{2(\log n + 4 \log(n/\alpha)S)T} + \alpha T
\end{equation*}
Now, it is easy to see that no $\alpha = \alpha(T)$ can balance $\sqrt{(-\log \alpha)ST}$ and $\alpha T$ to attain optimal bound $O(\sqrt{TS \log n})$. Therefore, we need more refined regret bound. Additionally, and more fundamentally, there is no simple description of $\Pi_{\Delta_\alpha^{n-1}, \psi}$ (see \cite[Theorem 7]{Herbster2001Tracking} for detailed analysis), which the algorithm computationally more challenging. 

\begin{algorithm}
\caption{Exponentiated Gradient with Modification} \label{alg:modified-eg}
\begin{algorithmic}[1]
\Require{Time horizon $T$, initial value $\mathbf{x}_1 = (1/n, \dots, 1/n) \in \Delta^{n-1}$, step size $\eta$, parameter $\alpha$}
\For{$t=1, \dots,T$}
\State Player plays $\mathbf{x}_t$
\State Player suffers $f_t(\mathbf{x}_t)$ and observes $f_t$
\State Update 
\[\Tilde{\mathbf{x}}_{t+1, j} = \frac{\mathbf{x}_{t,j} \exp(-\eta [\nabla f_t(\mathbf{x})]_j)}{\sum_{j=1}^n \mathbf{x}_{t, j} \exp(-\eta [\nabla f_t(\mathbf{x})]_j)}
\]
\Statex [Projection]
\State Project $\mathbf{x}_{t+1} = \Pi_{\Delta_{\alpha}^{n-1}, \psi}(\Tilde{\mathbf{x}}_{t+1})$
\Statex [Fixed Share]
\State Update $\mathbf{x}_{t+1, j} = (1 - \alpha) \Tilde{\mathbf{x}}_{t+1, j} + \alpha/n $ 
\EndFor
\end{algorithmic}
\end{algorithm}

In fact, there is a simpler way to resolve the issue to approaching the boundary yet can resolve the issues related to projection method: \textbf{fixed-share} algorithm. First introduced in \cite{Herbster1995TrackingTB}, the algorithm performs a mixing process with uniform weight, as in line 6 of algorithm \ref{alg:modified-eg}. Conceptually speaking, what mixing process with uniform weight does is that it sets minimum weights to every expert's opinion. If the problem is non-stationary, that is, if the best expert changes as time progresses, solely relying on past data ($i.e.,$ using only multiplicative update) is not the best strategy. Instead, the minimum weight makes the player to take \textit{some} advice from every expert every time so that it can adapt to shift in best expert. Another viewpoint of the fixed share step is that the multiplicative update due to observation of past loss gets discounted by factor $1 - \alpha$ every time. This agrees with our intuition that in non-stationary environment, data from past is likely to be obsolete.

This proof of tracking regret bound of fixed share algorithm is inspired by \cite[Theorem 2]{CesaBianchi2012MirrorDM}.
\begin{thm} 
The tracking regret of fixed share algorithm satisfies the following bound:
\begin{equation*}
    Reg_T^D(\mathbf{u}) \leq \frac{S + 1}{\eta}\log n + \frac{1}{\eta} \log \frac{1}{\alpha^S(1-\alpha)^{T - S- 1}} + \frac{\eta}{2}T
\end{equation*}
\end{thm}
\begin{proof}
The idea of proof is similar to that of proposition \ref{thm:mirror-dynamic}. However, rather than making a crude estimate of the term, we extensively exploit the problem setting to get a tighter bound. 

From lemma \ref{lem:step-omd},
\begin{align*}
    f_t(\mathbf{x}_t) - f_t(\mathbf{u}_t) &\leq \frac{B_\psi(\mathbf{u}_t, \mathbf{x}_t) - B_\psi(\mathbf{u}_t, \Tilde{\mathbf{x}}_{t+1})}{\eta} + \frac{\eta}{2}
    \\
    &= \frac{1}{\eta} \left(B_\psi(\mathbf{u}_t, \mathbf{x}_t) - B_\psi(\mathbf{u}_{t-1}, \Tilde{\mathbf{x}}_{t})\right)
    \\
    &+ \frac{1}{\eta} \left(B_\psi(\mathbf{u}_{t-1}, \Tilde{\mathbf{x}}_{t}) - B_\psi(\mathbf{u}_t, \Tilde{\mathbf{x}}_{t+1}) \right) + \frac{\eta}{2}
\end{align*}
Notice second term of RHS is a part of a telescoping sequence. To bound the first term, we first examine
\begin{align*}
     \sum_{i=1}^n \left[\mathbf{u}_{t, i} \frac{1}{\log \mathbf{x}_{t, i}} - \mathbf{u}_{t-1, i}\frac{1}{\Tilde{\mathbf{x}}_t} \right]
     &= \sum_{\mathbf{u}_{t, i} \geq \mathbf{u}_{t-1, i}} \left((\mathbf{u}_{t, i} - \mathbf{u}_{t-1, i}) \log \frac{1}{\mathbf{x}_{t, i}} + \mathbf{u}_{t-1, i} \log \frac{\Tilde{\mathbf{x}}_{t, i}}{\mathbf{x}_{t, i}}\right)
     \\
     &+ \sum_{\mathbf{u}_{t, i} < \mathbf{u}_{t-1, i}}\left( (\mathbf{u}_{t, i} - \mathbf{u}_{t-1, i})\log \frac{1}{\Tilde{\mathbf{x}}_{t, i}} + \mathbf{u}_{t, i} \log \frac{\Tilde{\mathbf{x}}_{t, i}}{\mathbf{x}_{t, i}}\right)
     \\
     &\leq \sum_{\mathbf{u}_{t, i} \geq \mathbf{u}_{t-1, i}} \left((\mathbf{u}_{t, i} - \mathbf{u}_{t-1, i}) \log \frac{1}{\mathbf{x}_{t, i}} + \mathbf{u}_{t-1, i} \log \frac{\Tilde{\mathbf{x}}_{t, i}}{\mathbf{x}_{t, i}}\right)
\end{align*}
The inequality holds since if $\mathbf{u}_{t, i} < \mathbf{u}_{t-1, i}$, then $-\log\frac{1}{\mathbf{x}_{t, i}} < 0$ and $\mathbf{u}_{t, i} = 0$.

Since $\mathbf{x}_{t, i} \geq \alpha/n$ and $\mathbf{x}_{t, i}/\Tilde{\mathbf{x}}_{t, i} = (1 - \alpha) + \alpha / (n\Tilde{\mathbf{x}}_{t, i}) \geq 1 - \alpha$,
\begin{align*}
    \sum_{i=1}^n \left[\mathbf{u}_{t, i} \frac{1}{\log \mathbf{x}_{t, i}} - \mathbf{u}_{t-1, i}\frac{1}{\Tilde{\mathbf{x}}_t} \right] &\leq \sum_{\mathbf{u}_{t, i} \geq \mathbf{u}_{t-1, i}} (\mathbf{u}_{t, i} - \mathbf{u}_{t-1, i}) \log \frac{n}{\alpha} 
    \\
    &+ \sum_{\mathbf{u}_{t, i} \geq \mathbf{u}_{t-1, i}} \mathbf{u}_{t-1, i} \log \frac{1}{1 - \alpha}
    \\
    &= \frac{1}{2} \lVert \mathbf{u}_t - \mathbf{u}_{t-1} \rVert_1 \log \frac{n}{\alpha} 
    \\
    &+ \left(1 - \frac{1}{2} \lVert \mathbf{u}_t - \mathbf{u}_{t-1} \rVert_1 \right) \log \frac{1}{1 - \alpha}
\end{align*}
The summation terms in the RHS of inequality simply detects whether there is a shift between consecutive comparators, and the last equality is a simply restating the summations in terms of $\lVert \mathbf{u}_t - \mathbf{u}_{t-1} \rVert_1$. Using this bound, we get
\begin{align*}
B_\psi(\mathbf{u}_t, \mathbf{x}_t) - B_\psi(\mathbf{u}_{t-1}, \Tilde{\mathbf{x}}_t) &= \psi(\mathbf{u}_t) - \psi(\mathbf{u}_{t-1}) + \sum_{i=1}^n \left[\mathbf{u}_{t, i} \frac{1}{\log \mathbf{x}_{t, i}} - \mathbf{u}_{t-1, i}\frac{1}{\Tilde{\mathbf{x}}_t} \right] 
\\
&= \psi(\mathbf{u}_t) - \psi(\mathbf{u}_{t-1}) + \frac{1}{2} \lVert \mathbf{u}_t - \mathbf{u}_{t-1} \rVert_1 \log \frac{n}{\alpha} 
\\
&+ \left(1 - \frac{1}{2}\lVert \mathbf{u}_t - \mathbf{u}_{t-1} \rVert_1 \right)\log\frac{1}{1-\alpha}
\end{align*}

Now, taking sum of $f_t(\mathbf{x}_t) - f_t(\mathbf{u}_t)$ from $t = 1, \dots, T$ and organizing terms,
\begin{align*}
    \sum_{t=1}^T \eta(f_t(\mathbf{x}_t) - f_t(\mathbf{u}_t)) &\leq S \log \frac{n}{\alpha} + \left(T-1-S\right) \log \frac{1}{1-\alpha} 
    \\
    &+ \psi(\mathbf{u}_T) - \psi(\mathbf{u}_1) + B_\psi(\mathbf{u}_1, \mathbf{x}_1) - B_\psi(\mathbf{u}_T, \Tilde{\mathbf{x}}_{T+1}) + \frac{\eta^2 T}{2}
\end{align*}
Now conclude by noting that negative cross entropy is always non-negative (which bounds $\psi(\mathbf{u}_T) - B_\psi(\mathbf{u}_T, \Tilde{\mathbf{x}}_{T+1})$) and that $\mathbf{x}_1 = (1/n, \dots, 1/n)$.
\end{proof}

What is left is to tune $\alpha$ and $\eta$ so minimize the bound. Typically, $\alpha$ is chosen to be $S / (T - 1)$ (see \cite[Section~5.2]{CesaBianchi2006PredictionLA}). Optimizing with respect to $\eta$, we get the following regret bound that is almost minimax optimal where we have an additional $O(T^2H(\alpha))$ term.  
\begin{cor}\cite[Corollary 5.1]{CesaBianchi2006PredictionLA} \label{cor:fixed}
Taking $\alpha = S/(T - 1)$ and choosing $\eta$ optimally in terms of $S$, $i.e.,$
\begin{center}
    $\displaystyle \eta = \sqrt{\frac{2}{T}\left((S+1) \log n + (T-1) H(\alpha) \right)}$,
\end{center} fixed share algorithm attains regret bound
\begin{align*}
    Reg_T^D &\leq \sqrt{\frac{T}{2}\left((S+1)\log n + (T-1) H(\alpha) \right)}
    % \\
    % &\leq \sqrt{\frac{T}{2}\left((S+1)\log n + S \log \left(\frac{eT}{S}\right)\right)}
\end{align*}
where $H(p) = -p\log p - (1-p) \log (1-p)$ is a binary entropy function.
\end{cor}
  
One problem with this result is that the optimal learning rate depends on the number of shifts $S$, which is often not known a priori. We can use the controller algorithm \ref{ader:controller} to remove the dependency.

\begin{prop}
Let $\mathcal{H} = \{ \eta_i = 2^{i-1}\sqrt{2 \log n / T} \mid i = 1, \dots, N \}$, where $N = \lceil \frac{1}{2} \log_2 (T(1 + \log n))\rceil + 1$. For any comparator sequence $\mathbf{u} \in [n]^T$, algorithm \ref{ader:controller} with fixed share as worker algorithm and initial weight $w_{1, k} \propto \frac{1}{k(k+1)}$ guarantees regret bound
\begin{center}
$Reg_T^D = O(\sqrt{TS\log n +S\log(T/S)})$
\end{center}
\end{prop}
\begin{proof}
Let $\eta^*$ denote optimal step size in corollary \ref{cor:fixed}. Since $0 \leq S \leq T-1$ and $0 \leq H(\alpha) \leq 1$,
\begin{center}
    $\displaystyle \sqrt{\frac{2}{T} \log n} \leq \sqrt{2(\log n + 1)}$.
\end{center}
Notice that $\min \mathcal{H} = \sqrt{\frac{2}{T} \log n}$ and $\max \mathcal{H} > \sqrt{2 (\log n + 1)}$. Therefore, there exists $k \in \{1, \dots, N-1\}$, such that $\eta_k \leq \eta^* \leq \eta_{k+1} = 2\eta_k$. Therefore, the regret bound of fixed share algorithm with $\alpha = S / (T - 1)$ satisfies
\begin{align*}
    \sum_{t=1}^T f_t(\mathbf{x}_t^{\eta_k}) - f_t(\mathbf{u}_t) &\leq \frac{S+1}{\eta_k} \log n + \frac{(T-1)H(\alpha)}{\eta_k} + \frac{\eta_k}{2}T 
    \\
    & \leq \frac{2(S+1)}{\eta^*} \log n + \frac{2(T-1)H(\alpha)}{\eta^*} + \frac{\eta^*}{2}T
    \\
    &= \sqrt{2T((S+1) \log n + (T-1) H(\alpha)}
\end{align*}
Adding this bound with result in proposition \ref{lem:controller}, we obtain
\begin{align*}
    \sum_{t=1}^T f_t(\mathbf{x}_t) - f_t(\mathbf{u}_t) &\leq \sqrt{2T((S+1)\log n + (T-1)H(\alpha))} + \frac{\sqrt{2T}}{2}(1 - \log w_{1, k})
    \\
    &\leq\sqrt{2T((S+1)\log n + (T-1)H(\alpha))} + \frac{\sqrt{2T}}{2}(1 + 2 \log k)
\end{align*}
where we use that $w_{1, k} \geq 1/k^2$ in the last inequality. 

Now, using $k = \lfloor \frac{1}{2} \log_2 \left[ (S+1) + \frac{(T-1)H(\alpha)}{\log n}\right]\rfloor + 1$ and that $H(x) \leq x \log (e/x)$, we obtain regret bound of order 
\begin{center}
    $\displaystyle Reg_T^D = O(\sqrt{TS\log n +S\log(T/S)})$
\end{center}
\end{proof}

\chapter{Conclusion and Future Work}
\label{Chap4}
In this work, we investigate current progress in online learning and dynamic regret. We showed that algorithm \ref{ader:controller} guarantees minimax optimal regret bound without knowing any information about comparator sequence. We extend theorem \cite[Theorem 3]{Zhang2018AdaptiveOL}, which is proved only for online gradient descent and problem setting formulated in $l_2$ norm, so that under additional assumption can be generalized to online mirror descent and more general class of problem setting (theorem \ref{ader:bound}). We then employee the technique in algorithm \ref{ader:controller} to prediction with expert advice setting, and show that we can attain regret bound of order $O(\sqrt{TS\log n +S\log(T/S)})$. The main advancement from work of \cite{Herbster1995TrackingTB} is that we now do not need to know the number of shifts in comparing expert sequence beforehand.

We now list some ways in which this work can develop further. Algorithm \ref{ader:controller} requires $N$ worker algorithm to run in parallel, where $N = O(\log T)$. For large $T$, this can be computationally burdensome, and may require ineffective worker algorithm to keep on running for long time. It is worthwhile to investigate whether restarting the algorithm after some time period can help reduce the number of worker algorithms while keeping regret bound in same order. 

The main rationale for maintaining a collection of worker algorithms is to remove dependence of path length to optimal learning rate. Investigating possible methods for removing the dependence while keeping the computational cost lower will be a huge breakthrough in this field. 

Lastly, it would be interesting future work for applying same technique in algorithm \ref{ader:controller} to other specific online convex optimization problem. In particular, applying the technique to multi-armed bandit problem can be promising, as multi-armed bandit has close tie with prediction with expert advice. 

\small{Word Count: 6360}

\renewcommand{\bibname}{Bibliography}
\bibliographystyle{unsrt}
\bibliography{Bibliography.bib}

\end{document}